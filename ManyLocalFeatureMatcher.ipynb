{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagner1986/PapyrusTech/blob/main/ManyLocalFeatureMatcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7qDTBLJ0voC",
        "outputId": "edd54f30-ee98-4aba-b487-789dcc805de3"
      },
      "outputs": [],
      "source": [
        "# !pip install kornia\n",
        "# !pip install kornia_moons --no-deps\n",
        "#!pip install kornia\n",
        "#!pip install kornia_moons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gc9ZYCfX_b6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def count_correct_matches(matches_matrix):\n",
        "    n, _ = matches_matrix.shape\n",
        "    count = 0\n",
        "    for i in range(n):\n",
        "        max_match = np.max(matches_matrix[i])\n",
        "        if matches_matrix[i, i] >= max_match:\n",
        "            count += 1\n",
        "    return count,n\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False  # Desativar para garantir a reprodutibilidade\n",
        "\n",
        "@contextmanager\n",
        "def medir_tempo(label: str = \"Tempo de execução\"):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        end = time.perf_counter()\n",
        "        print(f\"{label}: {end - start} segundos\")\n",
        "\n",
        "def print_table(matrix):\n",
        "    num_rows, num_cols = matrix.shape\n",
        "\n",
        "    # Encontrando o número máximo de dígitos para alinhar corretamente\n",
        "    max_width = len(str(num_rows * num_cols))\n",
        "\n",
        "    # Imprimindo os índices das colunas\n",
        "    print(f\"    \".rjust(max_width + 3), end=\" |\")\n",
        "    for col_idx in range(num_cols):\n",
        "        print(f\" Col{col_idx} \".center(10), end=\" |\")\n",
        "    print()\n",
        "\n",
        "    # Imprimindo uma linha de separação\n",
        "    print(f\"    \".rjust(max_width + 5)+\"-\" * (max_width + 1 + 11 * num_cols))\n",
        "\n",
        "    # Imprimindo as linhas da tabela\n",
        "    for row_idx in range(num_rows):\n",
        "        print(f\" Row{row_idx}\".ljust(max_width + 1), end=\" |\")  # Adiciona um espaço extra na primeira célula\n",
        "        for col_idx in range(num_cols):\n",
        "            print(f\" {matrix[row_idx, col_idx]} \".center(10), end=\" |\")\n",
        "        print()\n",
        "\n",
        "# Usando o context manager para medir o tempo de execução\n",
        "with medir_tempo('Processamento concluído'):\n",
        "    for _ in range(1000000):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N8q0nXD0ljr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import kornia as K\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "from kornia import tensor_to_image\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_tensor(tensor):\n",
        "    \"\"\"\n",
        "    Plota um tensor PyTorch como uma imagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - tensor (torch.Tensor): Um tensor 2D para imagens em escala de cinza ou um tensor 3D para imagens RGB.\n",
        "    \"\"\"\n",
        "    if tensor.dim() == 2:\n",
        "        # Imagem em escala de cinza\n",
        "        plt.imshow(tensor, cmap='gray')\n",
        "    elif tensor.dim() == 3:\n",
        "        # Convertendo o tensor de CxHxW para HxWxC para plotagem\n",
        "        tensor = tensor.permute(1, 2, 0)\n",
        "        plt.imshow(tensor)\n",
        "    else:\n",
        "        raise ValueError(\"O tensor deve ser 2D (imagem em escala de cinza) ou 3D (imagem RGB).\")\n",
        "\n",
        "    plt.axis('off')  # Remove os eixos para uma visualização mais limpa\n",
        "    plt.show()\n",
        "\n",
        "def plot_image_with_keypoints(image, keypoints):\n",
        "    \"\"\"\n",
        "    Plota uma imagem e seus keypoints.\n",
        "\n",
        "    Parâmetros:\n",
        "    - image (torch.Tensor): A imagem original.\n",
        "    - keypoints (torch.Tensor): Os keypoints detectados.\n",
        "    \"\"\"\n",
        "    # Converter a imagem para o formato do matplotlib (H, W, C) e escala [0, 1] se necessário\n",
        "    if image.dim() == 3:\n",
        "        image = image.permute(1, 2, 0)\n",
        "    if torch.max(image) > 1:\n",
        "        image = image / 255.0\n",
        "\n",
        "    plt.imshow(image.cpu().numpy())\n",
        "    if keypoints.dim() == 3:\n",
        "        keypoints = keypoints[0]  # Se os keypoints estiverem em um batch, pegue apenas o primeiro\n",
        "\n",
        "    # Plotar os keypoints\n",
        "    if keypoints.shape[1] == 2:\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=20, marker='.', c='r')\n",
        "    elif keypoints.shape[1] == 3:  # Se os keypoints incluírem a confiança\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=20 * keypoints[:, 2], marker='.', c='r')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh3EY4GfCi3A"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, List, Tuple\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class IPreprocessor(ABC):\n",
        "    \"\"\"\n",
        "    Interface para um pré-processador de imagens.\n",
        "    Define o contrato para classes que implementam operações de pré-processamento em imagens.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica operações de pré-processamento à imagem fornecida.\n",
        "\n",
        "        Parâmetros:\n",
        "            image (torch.Tensor): A imagem de entrada como um tensor do PyTorch.\n",
        "\n",
        "        Retorna:\n",
        "            torch.Tensor: A imagem após o pré-processamento.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalFeatureStructurer(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a estruturação global de features através de grafo.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, points: np.ndarray, featuresByPoints: np.ndarray) -> Tuple[np.ndarray,Any]:\n",
        "        \"\"\"\n",
        "        Estrutura features globais de uma imagem em um grafo, utilizando pontos e suas características associadas.\n",
        "\n",
        "        Args:\n",
        "            points (np.ndarray): Array de pontos extraídos de uma imagem, onde cada ponto é uma coordenada 2D.\n",
        "            featuresByPoints (np.ndarray): Array de características associadas a cada ponto.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Any]: Um grafo representando a estruturação global das features e o objeto Delaunay utilizado para a triangulação.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalMatcher(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a similaridade global de features através de grafo.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def __call__(self, matrixAdj0: np.ndarray, matrixAdj1: np.ndarray, threshold: float = 0.2) -> int:\n",
        "        \"\"\"\n",
        "        Interface para invocar operações de comparação de grafos.\n",
        "\n",
        "        Args:\n",
        "            matrixAdj0: A primeira matriz de adjacência do grafo.\n",
        "            matrixAdj1: A segunda matriz de adjacência do grafo.\n",
        "            threshold: O limiar para avaliar correspondências.\n",
        "\n",
        "        Returns:\n",
        "            Um valor representando a similaridade entre os dois grafos.\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ipAt_YaFFAf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from kornia.feature import LocalFeatureMatcher, GFTTAffNetHardNet, DescriptorMatcher,KeyNetHardNet\n",
        "\n",
        "# Supondo que 'device' já esteja definido, por exemplo, device = torch.device('cuda' ou 'cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)\n",
        "# Carregando a imagem, convertendo para float e normalizando\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(torch.float) / 255.0  # Normaliza para [0, 1]\n",
        "# plot_tensor(image1)\n",
        "\n",
        "# image1 = image1.to(device)\n",
        "# print(image1.shape)\n",
        "# gftt_hardnet_matcher = LocalFeatureMatcher(\n",
        "#     GFTTAffNetHardNet(25,device=device), DescriptorMatcher('smnn', 0.8)\n",
        "# )\n",
        "\n",
        "# input = {\"image0\": image1[:1][None], \"image1\": image1[:1][None]}# so funciona em escala de cinza\n",
        "# out = gftt_hardnet_matcher(input)\n",
        "# print(type(image1.cpu().data))\n",
        "# plot_image_with_keypoints(image1.cpu(),out['keypoints0'].detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhK7wsAQT1ZV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from kornia.feature import LocalFeature, DescriptorMatcher\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class PreprocessPipeline(IPreprocessor):\n",
        "    def __init__(self):\n",
        "        super(PreprocessPipeline, self).__init__()\n",
        "        self.transforms = nn.Sequential(\n",
        "            # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "            K.enhance.Normalize(0.0, 255.0),\n",
        "            # Redimensiona a imagem para o tamanho desejado\n",
        "            K.geometry.Resize((200, 200)),\n",
        "            # Converte a imagem RGB para escala de cinza , necessario comentar para plotar\n",
        "            K.color.RgbToGrayscale(),\n",
        "        )\n",
        "\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "        x = image#.float() / 255.0  # Garante que a operação de divisão é realizada em float\n",
        "        # Aplica as transformações definidas em __init__\n",
        "        x = self.transforms(x)\n",
        "        # Adiciona uma dimensão de batch no início se ainda não houver\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "class MyLocalFeatureMatcher:\n",
        "    def __init__(self, local_feature: LocalFeature, descriptor_matcher: DescriptorMatcher):\n",
        "        \"\"\"\n",
        "        Inicializa o matcher de características locais com um objeto LocalFeature para extração de características\n",
        "        e um objeto DescriptorMatcher para o matching de descritores.\n",
        "\n",
        "        Parâmetros:\n",
        "            local_feature (LocalFeature): Um objeto LocalFeature para extração combinada de detector e descritor.\n",
        "            descriptor_matcher (DescriptorMatcher): Um objeto DescriptorMatcher configurado para o matching de descritores.\n",
        "        \"\"\"\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "\n",
        "    def __call__(self, input) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extrai e match keypoints e descritores entre duas imagens.\n",
        "\n",
        "        Args:\n",
        "            image0: Primeira imagem.\n",
        "            image1: Segunda imagem.\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com keypoints, descritores e matches.\n",
        "        \"\"\"\n",
        "        image0 = input[\"image0\"]\n",
        "        image1 = input[\"image1\"]\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Delaunay\n",
        "class DelaunayGraph(IGlobalFeatureStructurer):\n",
        "    @staticmethod\n",
        "    def distancePoint(p1, p2):\n",
        "        return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
        "\n",
        "    def buildConnection(self,tri):\n",
        "        mapConnection = []\n",
        "        for simplice in tri.simplices:\n",
        "            mapConnection.extend([\n",
        "                [simplice[0], simplice[1]],\n",
        "                [simplice[1], simplice[2]],\n",
        "                [simplice[2], simplice[0]]\n",
        "            ])\n",
        "        return mapConnection\n",
        "\n",
        "    def buildMapGraph(self, mapConex,featuresByPoints):\n",
        "        size = len(featuresByPoints)\n",
        "        sample = np.matrix(np.ones((size, size)) * np.inf)\n",
        "        for i in range(size):\n",
        "            sample[i, i] = 0\n",
        "        for conexao in mapConex:\n",
        "            distance = self.distancePoint(featuresByPoints[conexao[0]], featuresByPoints[conexao[1]])\n",
        "            sample[conexao[0], conexao[1]] = distance\n",
        "        return sample\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_delaunay(points, tri):\n",
        "        plt.triplot(points[:, 0], points[:, 1], tri.simplices.copy(), color='orange')\n",
        "        plt.plot(points[:, 0], points[:, 1], 'o')\n",
        "        dist = 3\n",
        "        for i, point in enumerate(points):\n",
        "            plt.text(point[0] + dist, point[1] + dist, f' {i}', color='red', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, points, featuresByPoints):\n",
        "        # Assume-se que os pontos são extraídos de uma imagem e já processados\n",
        "        tri = Delaunay(points)\n",
        "        mapConnection = self.buildConnection(tri)\n",
        "        graph = self.buildMapGraph(mapConnection,featuresByPoints)\n",
        "        return graph,tri\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "class FloydWarshall(IGlobalMatcher):\n",
        "    @staticmethod\n",
        "    def floydWarshall(graph):\n",
        "        graph = csr_matrix(graph)\n",
        "        dist_matrix, _ = floyd_warshall(csgraph=graph, directed=False, return_predecessors=True)\n",
        "        return dist_matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def match_matrix(mat_a, mat_b, threshold):\n",
        "        mat_dist = mat_b - mat_a\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            mat_dist[i, :i] = 0  # Zera a metade inferior da matriz para evitar duplicação\n",
        "        mat_dist = mat_dist * mat_dist  # Eleva as diferenças ao quadrado\n",
        "        points = 0\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            for j in range(i + 1, mat_dist.shape[0]):\n",
        "                if mat_dist[i, j] < threshold:\n",
        "                    points += 1\n",
        "        return points\n",
        "\n",
        "    def __call__(self,matrixAdj0,matrixAdj1, threshold=0.2):\n",
        "        matAdjFull0 = self.floydWarshall(matrixAdj0)\n",
        "        matAdjFull1 = self.floydWarshall(matrixAdj1)\n",
        "        simGraph = self.match_matrix(matAdjFull0,matAdjFull1,threshold)\n",
        "        return simGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccx4czDmT1ZW"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)\n",
        "from kornia.feature import SIFTFeatureScaleSpace,SIFTFeature\n",
        "my_sift_scale_matcher = MyLocalFeatureMatcher(\n",
        "    SIFTFeatureScaleSpace(25,device=device), DescriptorMatcher('smnn', 0.8),#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        ")\n",
        "\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(device)\n",
        "# image2 = torchvision.io.read_image(\"image2.jpg\").to(device)\n",
        "# image3 = torchvision.io.read_image(\"image2.jpg\").to(device)\n",
        "# pp= PreprocessPipeline()\n",
        "\n",
        "\n",
        "# transform_original =torchvision.transforms.Compose([\n",
        "#             torchvision.transforms.ToPILImage(),\n",
        "#             torchvision.transforms.Resize((300, 300)),\n",
        "#             torchvision.transforms.PILToTensor(),\n",
        "#             torchvision.transforms.ConvertImageDtype(torch.float32),\n",
        "#         ])\n",
        "\n",
        "# with medir_tempo('Processamento concluído'):\n",
        "#   image1_p = transform_original(image1)\n",
        "#   image1_p = pp(image1_p)\n",
        "#   print(image1_p[:,:1].shape,image1_p[0].shape,image1.shape)\n",
        "#   input = {\"image0\": image1_p[:,:1], \"image1\":image1_p[:,:1]}\n",
        "#   out = my_sift_scale_matcher(input)\n",
        "\n",
        "# plot_image_with_keypoints(image1_p[0].cpu(),out['keypoints0'].detach().cpu())\n",
        "# out.keys()\n",
        "\n",
        "\n",
        "#avaliar com e sem gpu ok\n",
        "#avaliar o pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgCmI4N3YJn5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from kornia.feature import SIFTFeatureScaleSpace\n",
        "my_sift_scale_matcher = MyLocalFeatureMatcher(\n",
        "    SIFTFeatureScaleSpace(10,device=device), DescriptorMatcher('smnn', 0.8),#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        ")\n",
        "pp= PreprocessPipeline()\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(device)\n",
        "#\n",
        "# with medir_tempo('Processamento concluído'):\n",
        "#   image1_p = pp(image1)\n",
        "#   print(image1_p[:,:1].shape,image1_p[0].shape,image1.shape)\n",
        "#   input = {\"image0\": image1_p[:,:1], \"image1\":image1_p[:,:1]}\n",
        "#   out = my_sift_scale_matcher(input)\n",
        "\n",
        "# plot_image_with_keypoints(image1_p[0].cpu(),out['keypoints0'].detach().cpu())\n",
        "# out.keys()\n",
        "\n",
        "\n",
        "#avaliar com e sem gpu\n",
        "#avaliar o pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evg78xz6B235"
      },
      "outputs": [],
      "source": [
        "delaunayG = DelaunayGraph()  # Inicializa o construtor de grafo de Delaunay\n",
        "# Compara as matrizes de distância dos dois grafos\n",
        "floyd = FloydWarshall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skCSdbXVCtI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor: IPreprocessor = None, local_feature_matcher: MyLocalFeatureMatcher = None, global_structurer: IGlobalFeatureStructurer = None, global_matcher: IGlobalMatcher = None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature_matcher = local_feature_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self,out,index):\n",
        "        pts = out[f'keypoints{index}'][out['matches'][:, index]].detach().numpy()  # Pontos de keypoints correspondentes\n",
        "        desc = out[f'descriptors{index}'][out['matches'][:, index]].detach().numpy()  # Descritores dos keypoints correspondentes\n",
        "        matrixAdj, tri = self.global_structurer(pts, desc)\n",
        "        return matrixAdj\n",
        "\n",
        "    def run(self, image1: Any, image2: Any,log=None) -> float:\n",
        "        print(image1.shape,image2.shape)\n",
        "        if not all([self.preprocessor, self.local_feature_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        image1_processed, image2_processed = map(self.preprocessor, (image1, image2))\n",
        "        print(image1_processed.shape,image1_processed[:,:1].shape)\n",
        "        input = {\"image0\": image1_processed[:,:1], \"image1\":image2_processed[:,:1]}\n",
        "        print(\"local_feature_matching\",image1_processed.shape,type(image1_processed))\n",
        "        # Realiza o matching de características entre as duas imagens processadas\n",
        "        out = self.local_feature_matcher(input)\n",
        "\n",
        "        if log is not None and log in ('DEBUG','INFO'):\n",
        "          print(out['keypoints1'].shape,type(out['keypoints1']),'\\n',\n",
        "                out['lafs1'].shape,type(out['lafs1']),'\\n',\n",
        "                out['descriptors1'].shape,type(out['descriptors1']),'\\n',\n",
        "                out['matches'].shape,)\n",
        "        try:\n",
        "          matrixAdj0 = self.process_global(out,0)\n",
        "          matrixAdj1 = self.process_global(out,1)\n",
        "          score = self.global_matcher(matrixAdj0, matrixAdj1, threshold=0.005)\n",
        "        except:\n",
        "          score = 0\n",
        "        return score\n",
        "\n",
        "pipeline = ImageComparisonPipeline(pp,my_sift_scale_matcher,delaunayG,floyd)\n",
        "# try:\n",
        "#     similarity_score = pipeline.run(image1, image1)\n",
        "#     print(f\"Similarity Score: {similarity_score}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Erro durante a execução do pipeline: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3-rct8YJNgM"
      },
      "outputs": [],
      "source": [
        "class MyDrawMatcher:\n",
        "    def __init__(self,draw_dict=None) -> None:\n",
        "      if draw_dict is None:\n",
        "        draw_dict = {\n",
        "            \"inlier_color\": (0.2, 1, 0.2),\n",
        "            \"tentative_color\": (1.0, 0.5, 1),\n",
        "            \"feature_color\": (0.6, 0.5, 0),\n",
        "            \"vertical\": False,\n",
        "        }\n",
        "      self.draw_dict = draw_dict\n",
        "\n",
        "    def __call__(self,img1_preprocessed, img2_preprocessed, output) -> None:\n",
        "            from kornia_moons.viz import draw_LAF_matches\n",
        "            # Use kornia.tensor_to_image to ensure the images are in the correct format for drawing\n",
        "            img1 = K.tensor_to_image(img1_preprocessed.squeeze())\n",
        "            img2 = K.tensor_to_image(img2_preprocessed.squeeze())\n",
        "            # Assumes draw_LAF_matches is accessible and compatible with provided arguments\n",
        "            draw_LAF_matches(\n",
        "                output['lafs0'].cpu(),\n",
        "                output['lafs1'].cpu(),\n",
        "                output['matches'].cpu(),\n",
        "                img1,\n",
        "                img2,\n",
        "                None,  # Or the inliers if available\n",
        "                self.draw_dict\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xC3mt__LJNgM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "\n",
        "\n",
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature', 'descriptor_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor=None, local_feature: LocalFeature = None, descriptor_matcher: DescriptorMatcher = None, global_structurer=None, global_matcher=None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self, out):\n",
        "        matricesAdj = []\n",
        "        for index in (0, 1):\n",
        "            pts = out[f'keypoints{index}'][out['matches'][:, index]].cpu().detach().numpy()\n",
        "            desc = out[f'descriptors{index}'][out['matches'][:, index]].cpu().detach().numpy()\n",
        "            matrixAdj, _ = self.global_structurer(pts, desc)\n",
        "            matricesAdj.append(matrixAdj)\n",
        "        return matricesAdj\n",
        "\n",
        "    def local_feature_matching(self, image0, image1):\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        # print('local_feature_matching ',lafs0.shape,lafs0.device)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        # print('local_feature_matching ',lafs1.shape,lafs1.device)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "    def run(self, inspection_images: torch.Tensor, reference_images: torch.Tensor,threshold=0.1, log=None,device=torch.device('cpu')) -> Dict[Tuple[int, int], float]:\n",
        "        if not all([self.preprocessor, self.local_feature, self.descriptor_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        n,m = inspection_images.shape[0],reference_images.shape[0]\n",
        "        scores = np.zeros((n, m))\n",
        "        count_match = np.zeros((n, m))\n",
        "        myDraw =MyDrawMatcher()\n",
        "        for i_index, i_image in enumerate(inspection_images):\n",
        "            lafs0, responses0, descriptors0 = self.local_feature(i_image[:1][None])\n",
        "            for r_index, r_image in enumerate(reference_images):\n",
        "\n",
        "                lafs1, responses1, descriptors1 = self.local_feature(r_image[:1][None])\n",
        "                distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "                out = {\n",
        "                \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "                \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "                \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "                \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "                \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "                \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "                \"matches\": matches,#[M, 2])\n",
        "                }\n",
        "\n",
        "                # out = self.local_feature_matching(i_image[:1][None], r_image[:1][None])\n",
        "\n",
        "                try:\n",
        "                    matricesAdj = self.process_global(out)\n",
        "                    score = self.global_matcher(*matricesAdj, threshold=threshold)\n",
        "                except Exception as e:\n",
        "                    score = 0\n",
        "                if log is not None and log in ('DEBUG') and i_index == r_index:\n",
        "                    print(i_index,r_index ,out['keypoints0'].shape,out['descriptors0'].shape,out['matches'].shape,)\n",
        "                    myDraw(i_image.cpu(), r_image.cpu(),out)\n",
        "\n",
        "                count_match[i_index, r_index] = matches.shape[0]\n",
        "                scores[i_index, r_index] = score\n",
        "\n",
        "        if log is not None and log in ('INFO'):\n",
        "            print_table(count_match)\n",
        "            print(count_correct_matches(count_match))\n",
        "            print_table(scores)\n",
        "            print(count_correct_matches(scores))\n",
        "\n",
        "        return count_match,scores\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import kornia.augmentation as KA\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from kornia.feature import GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        "#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace/ SIFTFeature(pior desempenho)\n",
        "\n",
        "pipeline = ImageComparisonPipeline(pp, GFTTAffNetHardNet(50,device=device), DescriptorMatcher('smnn', 1.5), delaunayG, floyd)\n",
        "\n",
        "\n",
        "transform_original = transforms.Compose([\n",
        "    transforms.Resize((180, 180)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_inspect = KA.AugmentationSequential(\n",
        "    # KA.RandomHorizontalFlip(p=0.5),\n",
        "    # KA.RandomVerticalFlip(p=0.5),\n",
        "    KA. RandomMedianBlur((3, 3), p = 1),\n",
        "    KA.RandomPerspective(0.3, p=0.75),\n",
        "    KA.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25, p=0.75),#brightness=0.5, contrast=0.5, saturation=0.5, hue=0.15, p=0.75\n",
        "    KA.RandomAffine(degrees=90, translate=(0.10, 0.10), scale=(0.95, 1.05), p=0.75),\n",
        "    same_on_batch=True,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device',device)\n",
        "flowers102Dataset = datasets.Flowers102(root='./data', split='train', download=True,transform=transform_original)\n",
        "flowers_loader = DataLoader(flowers102Dataset, batch_size=20, shuffle=True,)\n",
        "# for batch_idx, (original_batch, target) in enumerate(flowers_loader):\n",
        "#     original_batch = original_batch.to(device)\n",
        "#     reference_images = original_batch\n",
        "#     inspection_images =  transform_inspect(original_batch)\n",
        "#     with medir_tempo():\n",
        "#       matchs,scores = pipeline.run(inspection_images, original_batch,device=device,log='INFO')\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tempo de execução: 17.590347621000546 segundos\n",
            "Tempo de execução: 17.59572443900106 segundos\n",
            "Tempo de execução: 17.281087627001398 segundos\n",
            "Tempo de execução: 17.864927881999392 segundos\n",
            "Tempo de execução: 17.71316234599908 segundos\n",
            "Tempo de execução: 17.509508766999716 segundos\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from itertools import product\n",
        "\n",
        "def grid_search_pipeline(device, dataset_loader, param_grid):\n",
        "    all_results = []\n",
        "    transform_inspect = KA.AugmentationSequential(\n",
        "        # KA.RandomHorizontalFlip(p=0.5),\n",
        "        # KA.RandomVerticalFlip(p=0.5),\n",
        "        KA. RandomMedianBlur((3, 3), p = 1),\n",
        "        KA.RandomPerspective(0.3, p=0.75),\n",
        "        KA.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25, p=0.75),#brightness=0.5, contrast=0.5, saturation=0.5, hue=0.15, p=0.75\n",
        "        KA.RandomAffine(degrees=90, translate=(0.10, 0.10), scale=(0.95, 1.05), p=0.75),\n",
        "        same_on_batch=True,\n",
        "    )\n",
        "    for num_features, feature_local_class, distance, threshold in product(*param_grid.values()):\n",
        "        set_seed(42)\n",
        "        # Configuração específica do extrator de features com a classe e número de features\n",
        "        feature_extractor = feature_local_class(num_features, device=device)\n",
        "        \n",
        "        # Configurar a pipeline com os parâmetros atuais\n",
        "        pipeline = ImageComparisonPipeline(\n",
        "            pp, \n",
        "            feature_extractor, \n",
        "            DescriptorMatcher('smnn', distance), \n",
        "            delaunayG, \n",
        "            floyd,\n",
        "        )\n",
        "        \n",
        "        # Executar a pipeline e registrar resultados para o batch atual\n",
        "        for batch_idx, (original_batch, target) in enumerate(dataset_loader):\n",
        "            original_batch = original_batch.to(device)\n",
        "            inspection_images = transform_inspect(original_batch)\n",
        "            with medir_tempo():\n",
        "                matches, scores = pipeline.run(inspection_images, original_batch,threshold=threshold, device=device, log=None)\n",
        "            all_results.append({\n",
        "                'params': {\n",
        "                    'num_features': num_features,\n",
        "                    'feature_local_class': feature_local_class.__name__,\n",
        "                    'distance': distance,\n",
        "                    'threshold': threshold\n",
        "                },\n",
        "                'matches': count_correct_matches(matches),\n",
        "                'scores': count_correct_matches(scores),\n",
        "            })\n",
        "            break # Remover este break para executar em todo o dataset\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Grid de parâmetros\n",
        "param_grid = {\n",
        "    'num_features': [10, 15],   \n",
        "    'feature_local': [GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace],\n",
        "    'distance': [1.5, 1.75],\n",
        "    'threshold': [0.005,0.5]\n",
        "}\n",
        "\n",
        "# Supondo que 'flowers_loader' seja o DataLoader do seu dataset\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "all_results = grid_search_pipeline(device, flowers_loader, param_grid)\n",
        "for result in all_results:\n",
        "    print(\"Parâmetros:\", result['params'])\n",
        "    print(\"Scores:\", result['scores'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----------------------+----------+-----------+----------+----------+\n",
            "| Num Features |  Feature Local Class  | Distance | Threshold | Matches  |  Scores  |\n",
            "+--------------+-----------------------+----------+-----------+----------+----------+\n",
            "|      30      |   GFTTAffNetHardNet   |   0.95   |   0.005   | (25, 30) | (27, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   0.95   |    0.05   | (25, 30) | (26, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   0.95   |    0.5    | (25, 30) | (26, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.5    |   0.005   | (18, 30) | (17, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.5    |    0.05   | (18, 30) | (16, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.5    |    0.5    | (18, 30) | (18, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.75   |   0.005   | (18, 30) | (17, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.75   |    0.05   | (18, 30) | (16, 30) |\n",
            "|      30      |   GFTTAffNetHardNet   |   1.75   |    0.5    | (18, 30) | (18, 30) |\n",
            "|      30      |     KeyNetHardNet     |   0.95   |   0.005   | (28, 30) | (29, 30) |\n",
            "|      30      |     KeyNetHardNet     |   0.95   |    0.05   | (28, 30) | (28, 30) |\n",
            "|      30      |     KeyNetHardNet     |   0.95   |    0.5    | (28, 30) | (28, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.5    |   0.005   | (15, 30) | (12, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.5    |    0.05   | (15, 30) | (13, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.5    |    0.5    | (15, 30) | (15, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.75   |   0.005   | (15, 30) | (12, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.75   |    0.05   | (15, 30) | (13, 30) |\n",
            "|      30      |     KeyNetHardNet     |   1.75   |    0.5    | (15, 30) | (15, 30) |\n",
            "|      30      |      SIFTFeature      |   0.95   |   0.005   | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   0.95   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   0.95   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   1.5    |   0.005   | (30, 30) | (29, 30) |\n",
            "|      30      |      SIFTFeature      |   1.5    |    0.05   | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   1.5    |    0.5    | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   1.75   |   0.005   | (30, 30) | (29, 30) |\n",
            "|      30      |      SIFTFeature      |   1.75   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      30      |      SIFTFeature      |   1.75   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   0.95   |   0.005   | (30, 30) | (29, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   0.95   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   0.95   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.5    |   0.005   | (28, 30) | (28, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.5    |    0.05   | (28, 30) | (27, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.5    |    0.5    | (28, 30) | (28, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.75   |   0.005   | (28, 30) | (28, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.75   |    0.05   | (28, 30) | (27, 30) |\n",
            "|      30      | SIFTFeatureScaleSpace |   1.75   |    0.5    | (28, 30) | (28, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   0.95   |   0.005   | (27, 30) | (27, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   0.95   |    0.05   | (27, 30) | (27, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   0.95   |    0.5    | (27, 30) | (27, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.5    |   0.005   | (17, 30) | (17, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.5    |    0.05   | (17, 30) | (16, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.5    |    0.5    | (17, 30) | (17, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.75   |   0.005   | (17, 30) | (17, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.75   |    0.05   | (17, 30) | (16, 30) |\n",
            "|      50      |   GFTTAffNetHardNet   |   1.75   |    0.5    | (17, 30) | (17, 30) |\n",
            "|      50      |     KeyNetHardNet     |   0.95   |   0.005   | (30, 30) | (30, 30) |\n",
            "|      50      |     KeyNetHardNet     |   0.95   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      50      |     KeyNetHardNet     |   0.95   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.5    |   0.005   | (16, 30) | (22, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.5    |    0.05   | (16, 30) | (15, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.5    |    0.5    | (16, 30) | (15, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.75   |   0.005   | (16, 30) | (22, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.75   |    0.05   | (16, 30) | (15, 30) |\n",
            "|      50      |     KeyNetHardNet     |   1.75   |    0.5    | (16, 30) | (15, 30) |\n",
            "|      50      |      SIFTFeature      |   0.95   |   0.005   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   0.95   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   0.95   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.5    |   0.005   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.5    |    0.05   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.5    |    0.5    | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.75   |   0.005   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.75   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      50      |      SIFTFeature      |   1.75   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   0.95   |   0.005   | (30, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   0.95   |    0.05   | (30, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   0.95   |    0.5    | (30, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.5    |   0.005   | (29, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.5    |    0.05   | (29, 30) | (29, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.5    |    0.5    | (29, 30) | (29, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.75   |   0.005   | (29, 30) | (30, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.75   |    0.05   | (29, 30) | (29, 30) |\n",
            "|      50      | SIFTFeatureScaleSpace |   1.75   |    0.5    | (29, 30) | (29, 30) |\n",
            "+--------------+-----------------------+----------+-----------+----------+----------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "# Criando uma tabela com prettytable\n",
        "table = PrettyTable()\n",
        "\n",
        "# Adicionando cabeçalhos à tabela\n",
        "table.field_names = [\"Num Features\", \"Feature Local Class\", \"Distance\", \"Threshold\", \"Matches\", \"Scores\"]\n",
        "\n",
        "# Adicionando as linhas com os resultados\n",
        "for result in all_results:\n",
        "    params = result['params']\n",
        "    matches = result['matches']\n",
        "    scores = result['scores']\n",
        "    table.add_row([params['num_features'], params['feature_local_class'], params['distance'], params['threshold'], matches, scores])\n",
        "\n",
        "# Imprimindo a tabela\n",
        "print(table)\n",
        "# Gerar a string CSV da tabela\n",
        "csv_string = table.get_csv_string()\n",
        "\n",
        "# Salvar a string CSV em um arquivo\n",
        "with open('resultados_grid_search.csv', 'w') as csv_file:\n",
        "    csv_file.write(csv_string)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
