{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagner1986/PapyrusTech/blob/main/ManyLocalFeatureMatcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7qDTBLJ0voC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd54f30-ee98-4aba-b487-789dcc805de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.2-py2.py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m980.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m104.9/121.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install kornia\n",
        "!pip install kornia_moons --no-deps\n",
        "#!pip install kornia\n",
        "#!pip install kornia_moons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gc9ZYCfX_b6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "import torch\n",
        "import numpy as np\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False  # Desativar para garantir a reprodutibilidade\n",
        "\n",
        "@contextmanager\n",
        "def medir_tempo(label: str = \"Tempo de execução\"):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        end = time.perf_counter()\n",
        "        print(f\"{label}: {end - start} segundos\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def print_table(matrix):\n",
        "    num_rows, num_cols = matrix.shape\n",
        "\n",
        "    # Encontrando o número máximo de dígitos para alinhar corretamente\n",
        "    max_width = len(str(num_rows * num_cols))\n",
        "\n",
        "    # Imprimindo os índices das colunas\n",
        "    print(f\"    \".rjust(max_width + 3), end=\" |\")\n",
        "    for col_idx in range(num_cols):\n",
        "        print(f\" Col{col_idx} \".center(10), end=\" |\")\n",
        "    print()\n",
        "\n",
        "    # Imprimindo uma linha de separação\n",
        "    print(f\"    \".rjust(max_width + 5)+\"-\" * (max_width + 1 + 11 * num_cols))\n",
        "\n",
        "    # Imprimindo as linhas da tabela\n",
        "    for row_idx in range(num_rows):\n",
        "        print(f\" Row{row_idx}\".ljust(max_width + 1), end=\" |\")  # Adiciona um espaço extra na primeira célula\n",
        "        for col_idx in range(num_cols):\n",
        "            print(f\" {matrix[row_idx, col_idx]} \".center(10), end=\" |\")\n",
        "        print()\n",
        "\n",
        "# Usando o context manager para medir o tempo de execução\n",
        "with medir_tempo('Processamento concluído'):\n",
        "    for _ in range(1000000):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N8q0nXD0ljr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import kornia as K\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "from kornia import tensor_to_image\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_tensor(tensor):\n",
        "    \"\"\"\n",
        "    Plota um tensor PyTorch como uma imagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - tensor (torch.Tensor): Um tensor 2D para imagens em escala de cinza ou um tensor 3D para imagens RGB.\n",
        "    \"\"\"\n",
        "    if tensor.dim() == 2:\n",
        "        # Imagem em escala de cinza\n",
        "        plt.imshow(tensor, cmap='gray')\n",
        "    elif tensor.dim() == 3:\n",
        "        # Convertendo o tensor de CxHxW para HxWxC para plotagem\n",
        "        tensor = tensor.permute(1, 2, 0)\n",
        "        plt.imshow(tensor)\n",
        "    else:\n",
        "        raise ValueError(\"O tensor deve ser 2D (imagem em escala de cinza) ou 3D (imagem RGB).\")\n",
        "\n",
        "    plt.axis('off')  # Remove os eixos para uma visualização mais limpa\n",
        "    plt.show()\n",
        "\n",
        "def plot_image_with_keypoints(image, keypoints):\n",
        "    \"\"\"\n",
        "    Plota uma imagem e seus keypoints.\n",
        "\n",
        "    Parâmetros:\n",
        "    - image (torch.Tensor): A imagem original.\n",
        "    - keypoints (torch.Tensor): Os keypoints detectados.\n",
        "    \"\"\"\n",
        "    # Converter a imagem para o formato do matplotlib (H, W, C) e escala [0, 1] se necessário\n",
        "    if image.dim() == 3:\n",
        "        image = image.permute(1, 2, 0)\n",
        "    if torch.max(image) > 1:\n",
        "        image = image / 255.0\n",
        "\n",
        "    plt.imshow(image.cpu().numpy())\n",
        "    if keypoints.dim() == 3:\n",
        "        keypoints = keypoints[0]  # Se os keypoints estiverem em um batch, pegue apenas o primeiro\n",
        "\n",
        "    # Plotar os keypoints\n",
        "    if keypoints.shape[1] == 2:\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=20, marker='.', c='r')\n",
        "    elif keypoints.shape[1] == 3:  # Se os keypoints incluírem a confiança\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=20 * keypoints[:, 2], marker='.', c='r')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh3EY4GfCi3A"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, List, Tuple\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class IPreprocessor(ABC):\n",
        "    \"\"\"\n",
        "    Interface para um pré-processador de imagens.\n",
        "    Define o contrato para classes que implementam operações de pré-processamento em imagens.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica operações de pré-processamento à imagem fornecida.\n",
        "\n",
        "        Parâmetros:\n",
        "            image (torch.Tensor): A imagem de entrada como um tensor do PyTorch.\n",
        "\n",
        "        Retorna:\n",
        "            torch.Tensor: A imagem após o pré-processamento.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalFeatureStructurer(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a estruturação global de features através de grafo.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, points: np.ndarray, featuresByPoints: np.ndarray) -> Tuple[np.ndarray,Any]:\n",
        "        \"\"\"\n",
        "        Estrutura features globais de uma imagem em um grafo, utilizando pontos e suas características associadas.\n",
        "\n",
        "        Args:\n",
        "            points (np.ndarray): Array de pontos extraídos de uma imagem, onde cada ponto é uma coordenada 2D.\n",
        "            featuresByPoints (np.ndarray): Array de características associadas a cada ponto.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Any]: Um grafo representando a estruturação global das features e o objeto Delaunay utilizado para a triangulação.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalMatcher(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a similaridade global de features através de grafo.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def __call__(self, matrixAdj0: np.ndarray, matrixAdj1: np.ndarray, threshold: float = 0.2) -> int:\n",
        "        \"\"\"\n",
        "        Interface para invocar operações de comparação de grafos.\n",
        "\n",
        "        Args:\n",
        "            matrixAdj0: A primeira matriz de adjacência do grafo.\n",
        "            matrixAdj1: A segunda matriz de adjacência do grafo.\n",
        "            threshold: O limiar para avaliar correspondências.\n",
        "\n",
        "        Returns:\n",
        "            Um valor representando a similaridade entre os dois grafos.\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ipAt_YaFFAf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from kornia.feature import LocalFeatureMatcher, GFTTAffNetHardNet, DescriptorMatcher,KeyNetHardNet\n",
        "\n",
        "# Supondo que 'device' já esteja definido, por exemplo, device = torch.device('cuda' ou 'cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)\n",
        "# Carregando a imagem, convertendo para float e normalizando\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(torch.float) / 255.0  # Normaliza para [0, 1]\n",
        "# plot_tensor(image1)\n",
        "\n",
        "# image1 = image1.to(device)\n",
        "# print(image1.shape)\n",
        "# gftt_hardnet_matcher = LocalFeatureMatcher(\n",
        "#     GFTTAffNetHardNet(25,device=device), DescriptorMatcher('smnn', 0.8)\n",
        "# )\n",
        "\n",
        "# input = {\"image0\": image1[:1][None], \"image1\": image1[:1][None]}# so funciona em escala de cinza\n",
        "# out = gftt_hardnet_matcher(input)\n",
        "# print(type(image1.cpu().data))\n",
        "# plot_image_with_keypoints(image1.cpu(),out['keypoints0'].detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhK7wsAQT1ZV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from kornia.feature import LocalFeature, DescriptorMatcher\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class PreprocessPipeline(IPreprocessor):\n",
        "    def __init__(self):\n",
        "        super(PreprocessPipeline, self).__init__()\n",
        "        self.transforms = nn.Sequential(\n",
        "            # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "            K.enhance.Normalize(0.0, 255.0),\n",
        "            # Redimensiona a imagem para o tamanho desejado\n",
        "            K.geometry.Resize((200, 200)),\n",
        "            # Converte a imagem RGB para escala de cinza , necessario comentar para plotar\n",
        "            K.color.RgbToGrayscale(),\n",
        "        )\n",
        "\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "        x = image#.float() / 255.0  # Garante que a operação de divisão é realizada em float\n",
        "        # Aplica as transformações definidas em __init__\n",
        "        x = self.transforms(x)\n",
        "        # Adiciona uma dimensão de batch no início se ainda não houver\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "class MyLocalFeatureMatcher:\n",
        "    def __init__(self, local_feature: LocalFeature, descriptor_matcher: DescriptorMatcher):\n",
        "        \"\"\"\n",
        "        Inicializa o matcher de características locais com um objeto LocalFeature para extração de características\n",
        "        e um objeto DescriptorMatcher para o matching de descritores.\n",
        "\n",
        "        Parâmetros:\n",
        "            local_feature (LocalFeature): Um objeto LocalFeature para extração combinada de detector e descritor.\n",
        "            descriptor_matcher (DescriptorMatcher): Um objeto DescriptorMatcher configurado para o matching de descritores.\n",
        "        \"\"\"\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "\n",
        "    def __call__(self, input) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extrai e match keypoints e descritores entre duas imagens.\n",
        "\n",
        "        Args:\n",
        "            image0: Primeira imagem.\n",
        "            image1: Segunda imagem.\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com keypoints, descritores e matches.\n",
        "        \"\"\"\n",
        "        image0 = input[\"image0\"]\n",
        "        image1 = input[\"image1\"]\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Delaunay\n",
        "class DelaunayGraph(IGlobalFeatureStructurer):\n",
        "    @staticmethod\n",
        "    def distancePoint(p1, p2):\n",
        "        return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
        "\n",
        "    def buildConnection(self,tri):\n",
        "        mapConnection = []\n",
        "        for simplice in tri.simplices:\n",
        "            mapConnection.extend([\n",
        "                [simplice[0], simplice[1]],\n",
        "                [simplice[1], simplice[2]],\n",
        "                [simplice[2], simplice[0]]\n",
        "            ])\n",
        "        return mapConnection\n",
        "\n",
        "    def buildMapGraph(self, mapConex,featuresByPoints):\n",
        "        size = len(featuresByPoints)\n",
        "        sample = np.matrix(np.ones((size, size)) * np.inf)\n",
        "        for i in range(size):\n",
        "            sample[i, i] = 0\n",
        "        for conexao in mapConex:\n",
        "            distance = self.distancePoint(featuresByPoints[conexao[0]], featuresByPoints[conexao[1]])\n",
        "            sample[conexao[0], conexao[1]] = distance\n",
        "        return sample\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_delaunay(points, tri):\n",
        "        plt.triplot(points[:, 0], points[:, 1], tri.simplices.copy(), color='orange')\n",
        "        plt.plot(points[:, 0], points[:, 1], 'o')\n",
        "        dist = 3\n",
        "        for i, point in enumerate(points):\n",
        "            plt.text(point[0] + dist, point[1] + dist, f' {i}', color='red', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, points, featuresByPoints):\n",
        "        # Assume-se que os pontos são extraídos de uma imagem e já processados\n",
        "        tri = Delaunay(points)\n",
        "        mapConnection = self.buildConnection(tri)\n",
        "        graph = self.buildMapGraph(mapConnection,featuresByPoints)\n",
        "        return graph,tri\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "class FloydWarshall(IGlobalMatcher):\n",
        "    @staticmethod\n",
        "    def floydWarshall(graph):\n",
        "        graph = csr_matrix(graph)\n",
        "        dist_matrix, _ = floyd_warshall(csgraph=graph, directed=False, return_predecessors=True)\n",
        "        return dist_matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def match_matrix(mat_a, mat_b, threshold):\n",
        "        mat_dist = mat_b - mat_a\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            mat_dist[i, :i] = 0  # Zera a metade inferior da matriz para evitar duplicação\n",
        "        mat_dist = mat_dist * mat_dist  # Eleva as diferenças ao quadrado\n",
        "        points = 0\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            for j in range(i + 1, mat_dist.shape[0]):\n",
        "                if mat_dist[i, j] < threshold:\n",
        "                    points += 1\n",
        "        return points\n",
        "\n",
        "    def __call__(self,matrixAdj0,matrixAdj1, threshold=0.2):\n",
        "        matAdjFull0 = self.floydWarshall(matrixAdj0)\n",
        "        matAdjFull1 = self.floydWarshall(matrixAdj1)\n",
        "        simGraph = self.match_matrix(matAdjFull0,matAdjFull1,threshold)\n",
        "        return simGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccx4czDmT1ZW"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "print(device)\n",
        "from kornia.feature import SIFTFeatureScaleSpace,SIFTFeature\n",
        "my_sift_scale_matcher = MyLocalFeatureMatcher(\n",
        "    SIFTFeatureScaleSpace(25,device=device), DescriptorMatcher('smnn', 0.8),#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        ")\n",
        "\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(device)\n",
        "# image2 = torchvision.io.read_image(\"image2.jpg\").to(device)\n",
        "# image3 = torchvision.io.read_image(\"image2.jpg\").to(device)\n",
        "# pp= PreprocessPipeline()\n",
        "\n",
        "\n",
        "# transform_original =torchvision.transforms.Compose([\n",
        "#             torchvision.transforms.ToPILImage(),\n",
        "#             torchvision.transforms.Resize((300, 300)),\n",
        "#             torchvision.transforms.PILToTensor(),\n",
        "#             torchvision.transforms.ConvertImageDtype(torch.float32),\n",
        "#         ])\n",
        "\n",
        "# with medir_tempo('Processamento concluído'):\n",
        "#   image1_p = transform_original(image1)\n",
        "#   image1_p = pp(image1_p)\n",
        "#   print(image1_p[:,:1].shape,image1_p[0].shape,image1.shape)\n",
        "#   input = {\"image0\": image1_p[:,:1], \"image1\":image1_p[:,:1]}\n",
        "#   out = my_sift_scale_matcher(input)\n",
        "\n",
        "# plot_image_with_keypoints(image1_p[0].cpu(),out['keypoints0'].detach().cpu())\n",
        "# out.keys()\n",
        "\n",
        "\n",
        "#avaliar com e sem gpu ok\n",
        "#avaliar o pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgCmI4N3YJn5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from kornia.feature import SIFTFeatureScaleSpace\n",
        "my_sift_scale_matcher = MyLocalFeatureMatcher(\n",
        "    SIFTFeatureScaleSpace(10,device=device), DescriptorMatcher('smnn', 0.8),#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        ")\n",
        "pp= PreprocessPipeline()\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").to(device)\n",
        "#\n",
        "# with medir_tempo('Processamento concluído'):\n",
        "#   image1_p = pp(image1)\n",
        "#   print(image1_p[:,:1].shape,image1_p[0].shape,image1.shape)\n",
        "#   input = {\"image0\": image1_p[:,:1], \"image1\":image1_p[:,:1]}\n",
        "#   out = my_sift_scale_matcher(input)\n",
        "\n",
        "# plot_image_with_keypoints(image1_p[0].cpu(),out['keypoints0'].detach().cpu())\n",
        "# out.keys()\n",
        "\n",
        "\n",
        "#avaliar com e sem gpu\n",
        "#avaliar o pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evg78xz6B235"
      },
      "outputs": [],
      "source": [
        "delaunayG = DelaunayGraph()  # Inicializa o construtor de grafo de Delaunay\n",
        "# Compara as matrizes de distância dos dois grafos\n",
        "floyd = FloydWarshall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skCSdbXVCtI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor: IPreprocessor = None, local_feature_matcher: MyLocalFeatureMatcher = None, global_structurer: IGlobalFeatureStructurer = None, global_matcher: IGlobalMatcher = None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature_matcher = local_feature_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self,out,index):\n",
        "        pts = out[f'keypoints{index}'][out['matches'][:, index]].detach().numpy()  # Pontos de keypoints correspondentes\n",
        "        desc = out[f'descriptors{index}'][out['matches'][:, index]].detach().numpy()  # Descritores dos keypoints correspondentes\n",
        "        matrixAdj, tri = self.global_structurer(pts, desc)\n",
        "        return matrixAdj\n",
        "\n",
        "    def run(self, image1: Any, image2: Any,log=None) -> float:\n",
        "        print(image1.shape,image2.shape)\n",
        "        if not all([self.preprocessor, self.local_feature_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        image1_processed, image2_processed = map(self.preprocessor, (image1, image2))\n",
        "        print(image1_processed.shape,image1_processed[:,:1].shape)\n",
        "        input = {\"image0\": image1_processed[:,:1], \"image1\":image2_processed[:,:1]}\n",
        "        print(\"local_feature_matching\",image1_processed.shape,type(image1_processed))\n",
        "        # Realiza o matching de características entre as duas imagens processadas\n",
        "        out = self.local_feature_matcher(input)\n",
        "\n",
        "        if log is not None and log in ('DEBUG','INFO'):\n",
        "          print(out['keypoints1'].shape,type(out['keypoints1']),'\\n',\n",
        "                out['lafs1'].shape,type(out['lafs1']),'\\n',\n",
        "                out['descriptors1'].shape,type(out['descriptors1']),'\\n',\n",
        "                out['matches'].shape,)\n",
        "        try:\n",
        "          matrixAdj0 = self.process_global(out,0)\n",
        "          matrixAdj1 = self.process_global(out,1)\n",
        "          score = self.global_matcher(matrixAdj0, matrixAdj1, threshold=0.005)\n",
        "        except:\n",
        "          score = 0\n",
        "        return score\n",
        "\n",
        "pipeline = ImageComparisonPipeline(pp,my_sift_scale_matcher,delaunayG,floyd)\n",
        "# try:\n",
        "#     similarity_score = pipeline.run(image1, image1)\n",
        "#     print(f\"Similarity Score: {similarity_score}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Erro durante a execução do pipeline: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3-rct8YJNgM"
      },
      "outputs": [],
      "source": [
        "class MyDrawMatcher:\n",
        "    def __init__(self,draw_dict=None) -> None:\n",
        "      if draw_dict is None:\n",
        "        draw_dict = {\n",
        "            \"inlier_color\": (0.2, 1, 0.2),\n",
        "            \"tentative_color\": (1.0, 0.5, 1),\n",
        "            \"feature_color\": (0.6, 0.5, 0),\n",
        "            \"vertical\": False,\n",
        "        }\n",
        "      self.draw_dict = draw_dict\n",
        "\n",
        "    def __call__(self,img1_preprocessed, img2_preprocessed, output) -> None:\n",
        "            from kornia_moons.viz import draw_LAF_matches\n",
        "            # Use kornia.tensor_to_image to ensure the images are in the correct format for drawing\n",
        "            img1 = K.tensor_to_image(img1_preprocessed.squeeze())\n",
        "            img2 = K.tensor_to_image(img2_preprocessed.squeeze())\n",
        "            # Assumes draw_LAF_matches is accessible and compatible with provided arguments\n",
        "            draw_LAF_matches(\n",
        "                output['lafs0'].cpu(),\n",
        "                output['lafs1'].cpu(),\n",
        "                output['matches'].cpu(),\n",
        "                img1,\n",
        "                img2,\n",
        "                None,  # Or the inliers if available\n",
        "                self.draw_dict\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC3mt__LJNgM"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "\n",
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature', 'descriptor_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor=None, local_feature: LocalFeature = None, descriptor_matcher: DescriptorMatcher = None, global_structurer=None, global_matcher=None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self, out):\n",
        "        matricesAdj = []\n",
        "        for index in (0, 1):\n",
        "            pts = out[f'keypoints{index}'][out['matches'][:, index]].cpu().detach().numpy()\n",
        "            desc = out[f'descriptors{index}'][out['matches'][:, index]].cpu().detach().numpy()\n",
        "            matrixAdj, _ = self.global_structurer(pts, desc)\n",
        "            matricesAdj.append(matrixAdj)\n",
        "        return matricesAdj\n",
        "\n",
        "    def local_feature_matching(self, image0, image1):\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        # print('local_feature_matching ',lafs0.shape,lafs0.device)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        # print('local_feature_matching ',lafs1.shape,lafs1.device)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "    def run(self, inspection_images: torch.Tensor, reference_images: torch.Tensor, log=None,device=torch.device('cpu')) -> Dict[Tuple[int, int], float]:\n",
        "        if not all([self.preprocessor, self.local_feature, self.descriptor_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        n,m = inspection_images.shape[0],reference_images.shape[0]\n",
        "        scores = np.zeros((n, m))\n",
        "        count_match = np.zeros((n, m))\n",
        "        myDraw =MyDrawMatcher()\n",
        "        print(inspection_images.shape,reference_images.shape)\n",
        "        for i_index, i_image in enumerate(inspection_images):\n",
        "            lafs0, responses0, descriptors0 = self.local_feature(i_image[:1][None])\n",
        "            for r_index, r_image in enumerate(reference_images):\n",
        "\n",
        "                lafs1, responses1, descriptors1 = self.local_feature(r_image[:1][None])\n",
        "                distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "                out = {\n",
        "                \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "                \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "                \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "                \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "                \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "                \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "                \"matches\": matches,#[M, 2])\n",
        "                }\n",
        "\n",
        "                # out = self.local_feature_matching(i_image[:1][None], r_image[:1][None])\n",
        "\n",
        "                try:\n",
        "                    matricesAdj = self.process_global(out)\n",
        "                    score = self.global_matcher(*matricesAdj, threshold=0.005)\n",
        "                except Exception as e:\n",
        "                    score = 0\n",
        "                if log is not None and log in ('DEBUG') and i_index == r_index:\n",
        "                    print(i_index,r_index ,out['keypoints0'].shape,out['descriptors0'].shape,out['matches'].shape,)\n",
        "                    myDraw(i_image.cpu(), r_image.cpu(),out)\n",
        "\n",
        "                count_match[i_index, r_index] = matches.shape[0]\n",
        "                scores[i_index, r_index] = score\n",
        "\n",
        "        if log is not None and log in ('INFO'):\n",
        "            print_table(count_match)\n",
        "            print_table(scores)\n",
        "\n",
        "        return count_match,scores\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import kornia.augmentation as KA\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from kornia.feature import GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        "#GFTTAffNetHardNet, KeyNetHardNet, SIFTFeature, SIFTFeatureScaleSpace\n",
        "\n",
        "pipeline = ImageComparisonPipeline(pp, GFTTAffNetHardNet(75,device=device), DescriptorMatcher('smnn', 1.70), delaunayG, floyd)\n",
        "\n",
        "\n",
        "transform_original = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_inspect = KA.AugmentationSequential(\n",
        "    # KA.RandomHorizontalFlip(p=0.5),\n",
        "    # KA.RandomVerticalFlip(p=0.5),\n",
        "    KA.RandomPerspective(0.3, p=0.75),\n",
        "    KA.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.25, p=0.75),\n",
        "    KA.RandomAffine(degrees=90, translate=(0.25, 0.25), scale=(0.90, 1.10), p=0.75),\n",
        "    same_on_batch=False,\n",
        ")\n",
        "\n",
        "\n",
        "flowers102Dataset = datasets.Flowers102(root='./data', split='train', download=True,transform=transform_original)\n",
        "flowers_loader = DataLoader(flowers102Dataset, batch_size=6, shuffle=True,)\n",
        "for batch_idx, (original_batch, target) in enumerate(flowers_loader):\n",
        "    original_batch = original_batch.to(device)\n",
        "    reference_images = original_batch\n",
        "    inspection_images =  transform_inspect(original_batch)\n",
        "    with medir_tempo():\n",
        "      matchs,scores = pipeline.run(inspection_images, original_batch,device=device,log='INFO')\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObJFuTY4JNgM"
      },
      "source": [
        "### checar se consiguimos fazer a deteccao do batch (OK)\n",
        "### verificar se faz necessario uso de cache (OK)\n",
        "### verificar a acuracia em varios cenarios (OK)\n",
        "### imprimir de maneira organizada o score floywarshall\n",
        "### imprimir de maneira organizada o num de matches"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}