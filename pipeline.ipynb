{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagner1986/PapyrusTech/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q1d1jTGiASK",
        "outputId": "3bbc9e4d-c4b2-4287-ce2a-57a3a64c8d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kornia in d:\\papyrustech\\venv\\lib\\site-packages (0.7.2)\n",
            "Requirement already satisfied: kornia-rs>=0.1.0 in d:\\papyrustech\\venv\\lib\\site-packages (from kornia) (0.1.1)\n",
            "Requirement already satisfied: packaging in d:\\papyrustech\\venv\\lib\\site-packages (from kornia) (24.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in d:\\papyrustech\\venv\\lib\\site-packages (from kornia) (2.2.1)\n",
            "Requirement already satisfied: filelock in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (4.10.0)\n",
            "Requirement already satisfied: sympy in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
            "Requirement already satisfied: fsspec in d:\\papyrustech\\venv\\lib\\site-packages (from torch>=1.9.1->kornia) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\papyrustech\\venv\\lib\\site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\papyrustech\\venv\\lib\\site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "O sistema n�o pode encontrar o caminho especificado.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kornia_moons in d:\\papyrustech\\venv\\lib\\site-packages (0.2.9)\n",
            "Requirement already satisfied: kornia in d:\\papyrustech\\venv\\lib\\site-packages (from kornia_moons) (0.7.2)\n",
            "Requirement already satisfied: torch in d:\\papyrustech\\venv\\lib\\site-packages (from kornia_moons) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in d:\\papyrustech\\venv\\lib\\site-packages (from kornia_moons) (3.8.3)\n",
            "Requirement already satisfied: opencv-python in d:\\papyrustech\\venv\\lib\\site-packages (from kornia_moons) (4.9.0.80)\n",
            "Requirement already satisfied: kornia-rs>=0.1.0 in d:\\papyrustech\\venv\\lib\\site-packages (from kornia->kornia_moons) (0.1.1)\n",
            "Requirement already satisfied: packaging in d:\\papyrustech\\venv\\lib\\site-packages (from kornia->kornia_moons) (24.0)\n",
            "Requirement already satisfied: filelock in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (4.10.0)\n",
            "Requirement already satisfied: sympy in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (3.1.3)\n",
            "Requirement already satisfied: fsspec in d:\\papyrustech\\venv\\lib\\site-packages (from torch->kornia_moons) (2024.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\papyrustech\\venv\\lib\\site-packages (from matplotlib->kornia_moons) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\papyrustech\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->kornia_moons) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\papyrustech\\venv\\lib\\site-packages (from jinja2->torch->kornia_moons) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\papyrustech\\venv\\lib\\site-packages (from sympy->torch->kornia_moons) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "O sistema n�o pode encontrar o caminho especificado.\n"
          ]
        }
      ],
      "source": [
        "#!pip install kornia\n",
        "#!pip install kornia_moons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs8g6hiWAkw9",
        "outputId": "c9216f86-cf45-4ca2-ceb6-02a91733dbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processamento concluído: 0.047876600000563485 segundos\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def medir_tempo(label: str = \"Tempo de execução\"):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        end = time.perf_counter()\n",
        "        print(f\"{label}: {end - start} segundos\")\n",
        "\n",
        "# Usando o context manager para medir o tempo de execução\n",
        "with medir_tempo('Processamento concluído'):\n",
        "    for _ in range(1000000):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QipVZV4bpT5S"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, List, Tuple\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class IPreprocessor(ABC):\n",
        "    \"\"\"\n",
        "    Interface para um pré-processador de imagens.\n",
        "    Define o contrato para classes que implementam operações de pré-processamento em imagens.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica operações de pré-processamento à imagem fornecida.\n",
        "\n",
        "        Parâmetros:\n",
        "            image (torch.Tensor): A imagem de entrada como um tensor do PyTorch.\n",
        "\n",
        "        Retorna:\n",
        "            torch.Tensor: A imagem após o pré-processamento.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalFeatureStructurer(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a estruturação global de features através de grafo.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, points: np.ndarray, featuresByPoints: np.ndarray) -> Tuple[np.ndarray,Any]:\n",
        "        \"\"\"\n",
        "        Estrutura features globais de uma imagem em um grafo, utilizando pontos e suas características associadas.\n",
        "\n",
        "        Args:\n",
        "            points (np.ndarray): Array de pontos extraídos de uma imagem, onde cada ponto é uma coordenada 2D.\n",
        "            featuresByPoints (np.ndarray): Array de características associadas a cada ponto.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Any]: Um grafo representando a estruturação global das features e o objeto Delaunay utilizado para a triangulação.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalMatcher(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a similaridade global de features através de grafo.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def __call__(self, matrixAdj0: np.ndarray, matrixAdj1: np.ndarray, threshold: float = 0.2) -> int:\n",
        "        \"\"\"\n",
        "        Interface para invocar operações de comparação de grafos.\n",
        "\n",
        "        Args:\n",
        "            matrixAdj0: A primeira matriz de adjacência do grafo.\n",
        "            matrixAdj1: A segunda matriz de adjacência do grafo.\n",
        "            threshold: O limiar para avaliar correspondências.\n",
        "\n",
        "        Returns:\n",
        "            Um valor representando a similaridade entre os dois grafos.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "import torch\n",
        "from kornia.feature import LocalFeature, DescriptorMatcher\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class MyLocalFeatureMatcher:\n",
        "    def __init__(self, local_feature: LocalFeature, descriptor_matcher: DescriptorMatcher):\n",
        "        \"\"\"\n",
        "        Inicializa o matcher de características locais com um objeto LocalFeature para extração de características\n",
        "        e um objeto DescriptorMatcher para o matching de descritores.\n",
        "\n",
        "        Parâmetros:\n",
        "            local_feature (LocalFeature): Um objeto LocalFeature para extração combinada de detector e descritor.\n",
        "            descriptor_matcher (DescriptorMatcher): Um objeto DescriptorMatcher configurado para o matching de descritores.\n",
        "        \"\"\"\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "\n",
        "    def __call__(self, image0: torch.Tensor, image1: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extrai e match keypoints e descritores entre duas imagens.\n",
        "\n",
        "        Args:\n",
        "            image0: Primeira imagem.\n",
        "            image1: Segunda imagem.\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com keypoints, descritores e matches.\n",
        "        \"\"\"\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "\n",
        "class MyDrawMatcher:\n",
        "  def __init__(self,draw_dict=None) -> None:\n",
        "      if draw_dict is None:\n",
        "        draw_dict = {\n",
        "            \"inlier_color\": (0.2, 1, 0.2),\n",
        "            \"tentative_color\": (1.0, 0.5, 1),\n",
        "            \"feature_color\": (0.6, 0.5, 0),\n",
        "            \"vertical\": False,\n",
        "        }\n",
        "      self.draw_dict = draw_dict\n",
        "\n",
        "  def __call__(self,img1_preprocessed, img2_preprocessed, output) -> None:\n",
        "        from kornia_moons.viz import draw_LAF_matches\n",
        "        # Use kornia.tensor_to_image to ensure the images are in the correct format for drawing\n",
        "        img1 = K.tensor_to_image(img1_preprocessed.squeeze())\n",
        "        img2 = K.tensor_to_image(img2_preprocessed.squeeze())\n",
        "        # Assumes draw_LAF_matches is accessible and compatible with provided arguments\n",
        "        draw_LAF_matches(\n",
        "            output['lafs0'],\n",
        "            output['lafs1'],\n",
        "            output['matches'],\n",
        "            img1,\n",
        "            img2,\n",
        "            None,  # Or the inliers if available\n",
        "            self.draw_dict\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uC7s1MDxpcXs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import kornia as K\n",
        "\n",
        "class PreprocessPipeline(IPreprocessor):\n",
        "    def __init__(self):\n",
        "        super(PreprocessPipeline, self).__init__()\n",
        "        self.transforms = nn.Sequential(\n",
        "            # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "            K.enhance.Normalize(0.0, 255.0),\n",
        "            # Redimensiona a imagem para o tamanho desejado\n",
        "            # K.geometry.Resize((200, 200)),\n",
        "            # Converte a imagem RGB para escala de cinza\n",
        "            K.color.RgbToGrayscale(),\n",
        "        )\n",
        "\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "        x = image.float() / 255.0  # Garante que a operação de divisão é realizada em float\n",
        "        # Aplica as transformações definidas em __init__\n",
        "        x = self.transforms(x)\n",
        "        # Adiciona uma dimensão de batch no início se ainda não houver\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Delaunay\n",
        "class DelaunayGraph(IGlobalFeatureStructurer):\n",
        "    @staticmethod\n",
        "    def distancePoint(p1, p2):\n",
        "        return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
        "\n",
        "    def buildConnection(self,tri):\n",
        "        mapConnection = []\n",
        "        for simplice in tri.simplices:\n",
        "            mapConnection.extend([\n",
        "                [simplice[0], simplice[1]],\n",
        "                [simplice[1], simplice[2]],\n",
        "                [simplice[2], simplice[0]]\n",
        "            ])\n",
        "        return mapConnection\n",
        "\n",
        "    def buildMapGraph(self, mapConex,featuresByPoints):\n",
        "        size = len(featuresByPoints)\n",
        "        sample = np.matrix(np.ones((size, size)) * np.inf)\n",
        "        for i in range(size):\n",
        "            sample[i, i] = 0\n",
        "        for conexao in mapConex:\n",
        "            distance = self.distancePoint(featuresByPoints[conexao[0]], featuresByPoints[conexao[1]])\n",
        "            sample[conexao[0], conexao[1]] = distance\n",
        "        return sample\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_delaunay(points, tri):\n",
        "        plt.triplot(points[:, 0], points[:, 1], tri.simplices.copy(), color='orange')\n",
        "        plt.plot(points[:, 0], points[:, 1], 'o')\n",
        "        dist = 3\n",
        "        for i, point in enumerate(points):\n",
        "            plt.text(point[0] + dist, point[1] + dist, f' {i}', color='red', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, points, featuresByPoints):\n",
        "        # Assume-se que os pontos são extraídos de uma imagem e já processados\n",
        "        tri = Delaunay(points)\n",
        "        mapConnection = self.buildConnection(tri)\n",
        "        graph = self.buildMapGraph(mapConnection,featuresByPoints)\n",
        "        return graph,tri\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "class FloydWarshall(IGlobalMatcher):\n",
        "    @staticmethod\n",
        "    def floydWarshall(graph):\n",
        "        graph = csr_matrix(graph)\n",
        "        dist_matrix, _ = floyd_warshall(csgraph=graph, directed=False, return_predecessors=True)\n",
        "        return dist_matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def match_matrix(mat_a, mat_b, threshold):\n",
        "        mat_dist = mat_b - mat_a\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            mat_dist[i, :i] = 0  # Zera a metade inferior da matriz para evitar duplicação\n",
        "        mat_dist = mat_dist * mat_dist  # Eleva as diferenças ao quadrado\n",
        "        points = 0\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            for j in range(i + 1, mat_dist.shape[0]):\n",
        "                if mat_dist[i, j] < threshold:\n",
        "                    points += 1\n",
        "        return points\n",
        "\n",
        "    def __call__(self,matrixAdj0,matrixAdj1, threshold=0.2):\n",
        "        matAdjFull0 = self.floydWarshall(matrixAdj0)\n",
        "        matAdjFull1 = self.floydWarshall(matrixAdj1)\n",
        "        simGraph = self.match_matrix(matAdjFull0,matAdjFull1,threshold)\n",
        "        return simGraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vTPZO0Mwuucx"
      },
      "outputs": [],
      "source": [
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor: IPreprocessor = None, local_feature_matcher: MyLocalFeatureMatcher = None, global_structurer: IGlobalFeatureStructurer = None, global_matcher: IGlobalMatcher = None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature_matcher = local_feature_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self,out,index):\n",
        "        pts = out[f'keypoints{index}'][out['matches'][:, index]].detach().numpy()  # Pontos de keypoints correspondentes\n",
        "        desc = out[f'descriptors{index}'][out['matches'][:, index]].detach().numpy()  # Descritores dos keypoints correspondentes\n",
        "        matrixAdj, tri = self.global_structurer(pts, desc)\n",
        "        return matrixAdj\n",
        "\n",
        "    def run(self, image1: Any, image2: Any,log=None) -> float:\n",
        "        if not all([self.preprocessor, self.local_feature_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        image1_processed, image2_processed = map(self.preprocessor, (image1, image2))\n",
        "\n",
        "        # Realiza o matching de características entre as duas imagens processadas\n",
        "        out = self.local_feature_matcher(image1_processed,image2_processed)\n",
        "\n",
        "        if log is not None and log in ('DEBUG','INFO'):\n",
        "          print(out['keypoints1'].shape,type(out['keypoints1']),'\\n',\n",
        "                out['lafs1'].shape,type(out['lafs1']),'\\n',\n",
        "                out['descriptors1'].shape,type(out['descriptors1']),'\\n',\n",
        "                out['matches'].shape,)\n",
        "\n",
        "        if log is not None and log in ('DEBUG'):\n",
        "          draw = MyDrawMatcher()\n",
        "          draw(image1_processed,image2_processed,out)\n",
        "        try:\n",
        "          matrixAdj0 = self.process_global(out,0)\n",
        "          matrixAdj1 = self.process_global(out,1)\n",
        "          score = self.global_matcher(matrixAdj0, matrixAdj1, threshold=0.005)\n",
        "        except:\n",
        "          score = 0\n",
        "        return score\n",
        "\n",
        "\n",
        "class ImageSearchEvaluator:\n",
        "    def __init__(self, pipeline: ImageComparisonPipeline):\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def data_augmentation(self, dataset: List[str]) -> List[str]:\n",
        "        # Implemente a operação de data augmentation para gerar o dataset de inspeção\n",
        "        dataset_de_inspecao = dataset  # Aqui você deve implementar a lógica de data augmentation\n",
        "        return dataset_de_inspecao\n",
        "\n",
        "    def evaluate_accuracy(self, dataset_de_referencia: List[str], dataset_de_inspecao: List[str]) -> float:\n",
        "        total_correspondencias_corretas = 0\n",
        "        total_correspondencias = 0\n",
        "\n",
        "        for indice_imagem_inspecao, imagem_de_inspecao in enumerate(dataset_de_inspecao):\n",
        "            # Realize uma busca sobre o dataset de referência para encontrar a correspondência\n",
        "            melhor_correspondencia = None\n",
        "            melhor_score = float('-inf')\n",
        "            print(\"fase {}\".format(indice_imagem_inspecao))\n",
        "            for indice_imagem_referencia, imagem_de_referencia in enumerate(dataset_de_referencia):\n",
        "                try:\n",
        "                    similarity_score = self.pipeline.run(imagem_de_referencia, imagem_de_inspecao)\n",
        "                    if similarity_score > melhor_score:\n",
        "                        melhor_score = similarity_score\n",
        "                        melhor_correspondencia = indice_imagem_referencia\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "                print(indice_imagem_inspecao,melhor_correspondencia)\n",
        "\n",
        "            # Verifique se a correspondência encontrada está correta\n",
        "            if melhor_correspondencia == indice_imagem_inspecao:\n",
        "                total_correspondencias_corretas += 1\n",
        "            total_correspondencias += 1\n",
        "\n",
        "\n",
        "        # Compute a acurácia do pipeline na busca das correspondências corretas\n",
        "        accuracy = total_correspondencias_corretas / total_correspondencias if total_correspondencias > 0 else 0\n",
        "        return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lpNTUTLjHG5J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ducha-aiki/affnet/raw/master/pretrained/AffNet.pth\" to C:\\Users\\wlsantos/.cache\\torch\\hub\\checkpoints\\AffNet.pth\n",
            "100.0%\n",
            "Downloading: \"https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth\" to C:\\Users\\wlsantos/.cache\\torch\\hub\\checkpoints\\checkpoint_liberty_with_aug.pth\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "from kornia.feature import LocalFeatureMatcher,GFTTAffNetHardNet\n",
        "from kornia import tensor_to_image  # Importa função para converter tensor PyTorch em imagem numpy\n",
        "\n",
        "# Carregar as duas imagens utilizando torchvision\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").float()\n",
        "# image2 = torchvision.io.read_image(\"image2.jpg\").float()\n",
        "\n",
        "preprocess = PreprocessPipeline()\n",
        "drawMatch = MyDrawMatcher()\n",
        "# Configura o extrator de características locais e o matcher de descritores\n",
        "customFeatureMatcher = MyLocalFeatureMatcher(\n",
        "    GFTTAffNetHardNet(25), K.feature.DescriptorMatcher('smnn', 0.95)\n",
        ")\n",
        "\n",
        "delaunayG = DelaunayGraph()  # Inicializa o construtor de grafo de Delaunay\n",
        "# Compara as matrizes de distância dos dois grafos\n",
        "floyd = FloydWarshall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M6vqE1L8s_fN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Exemplo de uso:\n",
        "\n",
        "pipeline = ImageComparisonPipeline(preprocess,customFeatureMatcher,delaunayG,floyd)\n",
        "\n",
        "# try:\n",
        "#     similarity_score = pipeline.run(image1, image2, log='DEBUG')\n",
        "#     print(f\"Similarity Score: {similarity_score}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "evaluator = ImageSearchEvaluator(pipeline)\n",
        "# Suponha que você tenha um dataset de referência e queira usar o mesmo dataset para gerar o dataset de inspeção\n",
        "# dataset_de_referencia = [image1,image2]  # Lista de tensores de imagem representando o dataset de referência\n",
        "# dataset_de_inspecao = evaluator.data_augmentation(dataset_de_referencia)\n",
        "\n",
        "# Calcule a acurácia do pipeline na busca das correspondências corretas\n",
        "# accuracy = evaluator.evaluate_accuracy(dataset_de_referencia, dataset_de_inspecao)\n",
        "# print(f\"Acurácia do pipeline na busca das correspondências corretas: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6pLVS531AVIC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data\\flowers-102\\102flowers.tgz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\flowers-102\\102flowers.tgz to data\\flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data\\flowers-102\\imagelabels.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data\\flowers-102\\setid.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import kornia.augmentation as KA\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Flowers102Dataset(Dataset):\n",
        "    def __init__(self, root='./data', split='train', download=True):\n",
        "        self.dataset = datasets.Flowers102(root=root, split=split, download=download, transform=transforms.ToTensor())\n",
        "        self.pre_transforms = transforms.Compose([\n",
        "            transforms.Resize((200,200))  # Redimensionamento\n",
        "        ])\n",
        "        # Transformações que serão aplicadas para obter a versão transformada\n",
        "        self.transforms = torch.nn.Sequential(\n",
        "            KA.RandomHorizontalFlip(p=0.5),\n",
        "            KA.RandomVerticalFlip(p=0.5),\n",
        "            # KA.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            KA.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.95, 1.05), p=0.5)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.dataset[index]\n",
        "\n",
        "        # Armazenando a versão original (antes das transformações)\n",
        "        original_data = self.pre_transforms(data)\n",
        "\n",
        "        # Aplicando transformações para obter a versão transformada\n",
        "        transformed_data = self.transforms(original_data.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # Retornando ambas as versões da imagem junto com o target/label\n",
        "        return original_data, transformed_data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "# DataLoader\n",
        "flowers_loader = DataLoader(Flowers102Dataset(), batch_size=24, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtyMXLAzAaIn",
        "outputId": "897cfcd4-b979-440c-e2fb-73c9d402559b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_idx 0  -  torch.Size([24, 3, 200, 200])\n",
            "Processamento concluído: 13.919254700000238 segundos\n",
            "Match 0 -> 0\n",
            "Processamento concluído: 11.382538900000327 segundos\n",
            "Match 1 -> 14\n",
            "Processamento concluído: 10.513866000000235 segundos\n",
            "Match 2 -> 2\n",
            "Processamento concluído: 11.554497999999512 segundos\n",
            "Match 3 -> 3\n",
            "Processamento concluído: 12.412414200000057 segundos\n",
            "Match 4 -> 4\n",
            "Processamento concluído: 11.40395830000034 segundos\n",
            "Match 5 -> 5\n",
            "Processamento concluído: 11.542033199999423 segundos\n",
            "Match 6 -> 6\n",
            "Processamento concluído: 12.542149199999585 segundos\n",
            "Match 7 -> 7\n",
            "Processamento concluído: 13.276217300000098 segundos\n",
            "Match 8 -> 8\n",
            "Processamento concluído: 13.960718399999678 segundos\n",
            "Match 9 -> 9\n",
            "Processamento concluído: 12.016569299999901 segundos\n",
            "Match 10 -> 10\n",
            "Processamento concluído: 11.888877200000024 segundos\n",
            "Match 11 -> 11\n",
            "Processamento concluído: 11.976757200000065 segundos\n",
            "Match 12 -> 12\n",
            "Processamento concluído: 11.624600199999804 segundos\n",
            "Match 13 -> 13\n",
            "Processamento concluído: 11.833872400000473 segundos\n",
            "Match 14 -> 14\n",
            "Processamento concluído: 11.230234399999972 segundos\n",
            "Match 15 -> 15\n",
            "Processamento concluído: 11.631305699999757 segundos\n",
            "Match 16 -> 5\n",
            "Processamento concluído: 11.583347900000263 segundos\n",
            "Match 17 -> 17\n",
            "Processamento concluído: 11.806513599999562 segundos\n",
            "Match 18 -> 22\n",
            "Processamento concluído: 11.639360699999997 segundos\n",
            "Match 19 -> 19\n",
            "Processamento concluído: 12.507757399999718 segundos\n",
            "accuracy ->  0.85\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 39\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     38\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ImageSearchEvaluator(pipeline)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflowers_loader\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[21], line 18\u001b[0m, in \u001b[0;36mImageSearchEvaluator.evaluate_accuracy\u001b[1;34m(self, flowers_loader)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indice_imagem_referencia, imagem_de_referencia \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(original_data):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m         similarity_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagem_de_inspecao\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagem_de_referencia\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m similarity_score \u001b[38;5;241m>\u001b[39m melhor_score:\n\u001b[0;32m     20\u001b[0m             melhor_score \u001b[38;5;241m=\u001b[39m similarity_score\n",
            "Cell \u001b[1;32mIn[17], line 22\u001b[0m, in \u001b[0;36mImageComparisonPipeline.run\u001b[1;34m(self, image1, image2, log)\u001b[0m\n\u001b[0;32m     19\u001b[0m image1_processed, image2_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor, (image1, image2))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Realiza o matching de características entre as duas imagens processadas\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_feature_matcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage2_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     25\u001b[0m   \u001b[38;5;28mprint\u001b[39m(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mtype\u001b[39m(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m         out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mtype\u001b[39m(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs1\u001b[39m\u001b[38;5;124m'\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m         out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mtype\u001b[39m(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors1\u001b[39m\u001b[38;5;124m'\u001b[39m]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m         out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,)\n",
            "Cell \u001b[1;32mIn[15], line 94\u001b[0m, in \u001b[0;36mMyLocalFeatureMatcher.__call__\u001b[1;34m(self, image0, image1)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03mExtrai e match keypoints e descritores entre duas imagens.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Dicionário com keypoints, descritores e matches.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m lafs0, responses0, descriptors0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_feature(image0)\n\u001b[1;32m---> 94\u001b[0m lafs1, responses1, descriptors1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m distance, matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescriptor_matcher(descriptors0[\u001b[38;5;241m0\u001b[39m], descriptors1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m\"\u001b[39m: lafs0[\u001b[38;5;241m0\u001b[39m, :, :, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdata,\u001b[38;5;66;03m#[N, 2])\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m\"\u001b[39m: lafs1[\u001b[38;5;241m0\u001b[39m, :, :, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdata,\u001b[38;5;66;03m#[N, 2])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m: matches,\u001b[38;5;66;03m#[M, 2])\u001b[39;00m\n\u001b[0;32m    105\u001b[0m }\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\integrated.py:139\u001b[0m, in \u001b[0;36mLocalFeature.forward\u001b[1;34m(self, img, mask)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        img: image to extract features with shape :math:`(B,C,H,W)`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m        - Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     lafs, responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     lafs \u001b[38;5;241m=\u001b[39m scale_laf(lafs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_coef)\n\u001b[0;32m    141\u001b[0m     descs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescriptor(img, lafs)\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\scale_space_detector.py:392\u001b[0m, in \u001b[0;36mMultiResolutionDetector.forward\u001b[1;34m(self, img, mask)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Three stage local feature detection. First the location and scale of interest points are determined by\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03mdetect function. Then affine shape and orientation.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    responses: shape [1xNx1]. Response function values for corresponding lafs\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    391\u001b[0m KORNIA_CHECK_SHAPE(img, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 392\u001b[0m responses, lafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m lafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maff(lafs, img)\n\u001b[0;32m    394\u001b[0m lafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori(lafs, img)\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\scale_space_detector.py:347\u001b[0m, in \u001b[0;36mMultiResolutionDetector.detect\u001b[1;34m(self, img, mask)\u001b[0m\n\u001b[0;32m    344\u001b[0m up_factor_kpts \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mfloat\u001b[39m(w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(nw), \u001b[38;5;28mfloat\u001b[39m(h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(nh))\n\u001b[0;32m    345\u001b[0m img_up \u001b[38;5;241m=\u001b[39m resize(img_up, (nh, nw), interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 347\u001b[0m cur_scores, cur_lafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_features_on_single_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_factor_kpts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m all_responses\u001b[38;5;241m.\u001b[39mappend(cur_scores\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    350\u001b[0m all_lafs\u001b[38;5;241m.\u001b[39mappend(cur_lafs)\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\scale_space_detector.py:302\u001b[0m, in \u001b[0;36mMultiResolutionDetector.detect_features_on_single_level\u001b[1;34m(self, level_img, num_kp, factor)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_features_on_single_level\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m, level_img: Tensor, num_kp: \u001b[38;5;28mint\u001b[39m, factor: Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]\n\u001b[0;32m    301\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m--> 302\u001b[0m     det_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_borders(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_img\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    303\u001b[0m     device \u001b[38;5;241m=\u001b[39m level_img\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    304\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m level_img\u001b[38;5;241m.\u001b[39mdtype\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\responses.py:371\u001b[0m, in \u001b[0;36mCornerGFTT.forward\u001b[1;34m(self, input, sigmas)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, sigmas: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgftt_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrads_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PapyrusTech\\venv\\Lib\\site-packages\\kornia\\feature\\responses.py:172\u001b[0m, in \u001b[0;36mgftt_response\u001b[1;34m(input, grads_mode, sigmas)\u001b[0m\n\u001b[0;32m    169\u001b[0m trace_m: Tensor \u001b[38;5;241m=\u001b[39m dx2 \u001b[38;5;241m+\u001b[39m dy2\n\u001b[0;32m    171\u001b[0m e1: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (trace_m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt((trace_m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m det_m)\u001b[38;5;241m.\u001b[39mabs()))\n\u001b[1;32m--> 172\u001b[0m e2: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (trace_m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43m(\u001b[49m\u001b[43mtrace_m\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdet_m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    174\u001b[0m scores: Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(e1, e2)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sigmas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class ImageSearchEvaluator:\n",
        "    def __init__(self, pipeline: ImageComparisonPipeline):\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def evaluate_accuracy(self, flowers_loader) -> float:\n",
        "        total_correspondencias_corretas = 0\n",
        "        total_correspondencias = 0\n",
        "        try:\n",
        "          for batch_idx, (original_data, transformed_data, target) in enumerate(flowers_loader):\n",
        "            print(\"batch_idx {}  -  {}\".format(batch_idx,original_data.shape))\n",
        "            for indice_imagem_inspecao, imagem_de_inspecao in enumerate(transformed_data):\n",
        "                # Realize uma busca sobre o dataset de referência para encontrar a correspondência\n",
        "                melhor_correspondencia = None\n",
        "                melhor_score = float('-inf')\n",
        "                with medir_tempo('Processamento concluído'):\n",
        "                  for indice_imagem_referencia, imagem_de_referencia in enumerate(original_data):\n",
        "                      try:\n",
        "                          similarity_score = self.pipeline.run(imagem_de_inspecao, imagem_de_referencia)\n",
        "                          if similarity_score > melhor_score:\n",
        "                              melhor_score = similarity_score\n",
        "                              melhor_correspondencia = indice_imagem_referencia\n",
        "                      except Exception as e:\n",
        "                          print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "\n",
        "                # Verifique se a correspondência encontrada está correta\n",
        "                if melhor_correspondencia == indice_imagem_inspecao:\n",
        "                    total_correspondencias_corretas += 1\n",
        "                total_correspondencias += 1\n",
        "                print(\"Match {} -> {}\".format(indice_imagem_inspecao,melhor_correspondencia))\n",
        "        finally:\n",
        "          # Compute a acurácia do pipeline na busca das correspondências corretas\n",
        "          accuracy = total_correspondencias_corretas / total_correspondencias if total_correspondencias > 0 else 0\n",
        "          print(\"accuracy -> \",accuracy)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "evaluator = ImageSearchEvaluator(pipeline)\n",
        "evaluator.evaluate_accuracy(flowers_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMU6xleQCDpS9NQSf75L6VU",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
