{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU6xleQCDpS9NQSf75L6VU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagner1986/PapyrusTech/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-k45N7iWfOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia\n",
        "!pip install kornia_moons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q1d1jTGiASK",
        "outputId": "3bbc9e4d-c4b2-4287-ce2a-57a3a64c8d2f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: kornia_moons in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (from kornia_moons) (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from kornia_moons) (2.1.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from kornia_moons) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from kornia_moons) (4.8.0.76)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia->kornia_moons) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->kornia_moons) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kornia_moons) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->kornia_moons) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->kornia_moons) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->kornia_moons) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def medir_tempo(label: str = \"Tempo de execução\"):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        end = time.perf_counter()\n",
        "        print(f\"{label}: {end - start} segundos\")\n",
        "\n",
        "# Usando o context manager para medir o tempo de execução\n",
        "with medir_tempo('Processamento concluído'):\n",
        "    for _ in range(1000000):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Fs8g6hiWAkw9",
        "outputId": "c9216f86-cf45-4ca2-ceb6-02a91733dbc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processamento concluído: 0.05225249199997961 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, List, Tuple\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class IPreprocessor(ABC):\n",
        "    \"\"\"\n",
        "    Interface para um pré-processador de imagens.\n",
        "    Define o contrato para classes que implementam operações de pré-processamento em imagens.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica operações de pré-processamento à imagem fornecida.\n",
        "\n",
        "        Parâmetros:\n",
        "            image (torch.Tensor): A imagem de entrada como um tensor do PyTorch.\n",
        "\n",
        "        Retorna:\n",
        "            torch.Tensor: A imagem após o pré-processamento.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalFeatureStructurer(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a estruturação global de features através de grafo.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, points: np.ndarray, featuresByPoints: np.ndarray) -> Tuple[np.ndarray,Any]:\n",
        "        \"\"\"\n",
        "        Estrutura features globais de uma imagem em um grafo, utilizando pontos e suas características associadas.\n",
        "\n",
        "        Args:\n",
        "            points (np.ndarray): Array de pontos extraídos de uma imagem, onde cada ponto é uma coordenada 2D.\n",
        "            featuresByPoints (np.ndarray): Array de características associadas a cada ponto.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Any]: Um grafo representando a estruturação global das features e o objeto Delaunay utilizado para a triangulação.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class IGlobalMatcher(ABC):\n",
        "    \"\"\"\n",
        "    Interface para a similaridade global de features através de grafo.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def __call__(self, matrixAdj0: np.ndarray, matrixAdj1: np.ndarray, threshold: float = 0.2) -> int:\n",
        "        \"\"\"\n",
        "        Interface para invocar operações de comparação de grafos.\n",
        "\n",
        "        Args:\n",
        "            matrixAdj0: A primeira matriz de adjacência do grafo.\n",
        "            matrixAdj1: A segunda matriz de adjacência do grafo.\n",
        "            threshold: O limiar para avaliar correspondências.\n",
        "\n",
        "        Returns:\n",
        "            Um valor representando a similaridade entre os dois grafos.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "import torch\n",
        "from kornia.feature import LocalFeature, DescriptorMatcher\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class MyLocalFeatureMatcher:\n",
        "    def __init__(self, local_feature: LocalFeature, descriptor_matcher: DescriptorMatcher):\n",
        "        \"\"\"\n",
        "        Inicializa o matcher de características locais com um objeto LocalFeature para extração de características\n",
        "        e um objeto DescriptorMatcher para o matching de descritores.\n",
        "\n",
        "        Parâmetros:\n",
        "            local_feature (LocalFeature): Um objeto LocalFeature para extração combinada de detector e descritor.\n",
        "            descriptor_matcher (DescriptorMatcher): Um objeto DescriptorMatcher configurado para o matching de descritores.\n",
        "        \"\"\"\n",
        "        self.local_feature = local_feature\n",
        "        self.descriptor_matcher = descriptor_matcher\n",
        "\n",
        "    def __call__(self, image0: torch.Tensor, image1: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extrai e match keypoints e descritores entre duas imagens.\n",
        "\n",
        "        Args:\n",
        "            image0: Primeira imagem.\n",
        "            image1: Segunda imagem.\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com keypoints, descritores e matches.\n",
        "        \"\"\"\n",
        "        lafs0, responses0, descriptors0 = self.local_feature(image0)\n",
        "        lafs1, responses1, descriptors1 = self.local_feature(image1)\n",
        "        distance, matches = self.descriptor_matcher(descriptors0[0], descriptors1[0])\n",
        "\n",
        "        return {\n",
        "            \"keypoints0\": lafs0[0, :, :, 2].data,#[N, 2])\n",
        "            \"keypoints1\": lafs1[0, :, :, 2].data,#[N, 2])\n",
        "            \"lafs0\": lafs0,#[1, N, 2, 3]\n",
        "            \"lafs1\": lafs1,#[1, N, 2, 3]\n",
        "            \"descriptors0\": descriptors0[0],#[N, 128])\n",
        "            \"descriptors1\": descriptors1[0],#[N, 128])\n",
        "            \"matches\": matches,#[M, 2])\n",
        "        }\n",
        "\n",
        "\n",
        "class MyDrawMatcher:\n",
        "  def __init__(self,draw_dict=None) -> None:\n",
        "      if draw_dict is None:\n",
        "        draw_dict = {\n",
        "            \"inlier_color\": (0.2, 1, 0.2),\n",
        "            \"tentative_color\": (1.0, 0.5, 1),\n",
        "            \"feature_color\": (0.6, 0.5, 0),\n",
        "            \"vertical\": False,\n",
        "        }\n",
        "      self.draw_dict = draw_dict\n",
        "\n",
        "  def __call__(self,img1_preprocessed, img2_preprocessed, output) -> None:\n",
        "        from kornia_moons.viz import draw_LAF_matches\n",
        "        # Use kornia.tensor_to_image to ensure the images are in the correct format for drawing\n",
        "        img1 = K.tensor_to_image(img1_preprocessed.squeeze())\n",
        "        img2 = K.tensor_to_image(img2_preprocessed.squeeze())\n",
        "        # Assumes draw_LAF_matches is accessible and compatible with provided arguments\n",
        "        draw_LAF_matches(\n",
        "            output['lafs0'],\n",
        "            output['lafs1'],\n",
        "            output['matches'],\n",
        "            img1,\n",
        "            img2,\n",
        "            None,  # Or the inliers if available\n",
        "            self.draw_dict\n",
        "        )"
      ],
      "metadata": {
        "id": "QipVZV4bpT5S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import kornia as K\n",
        "\n",
        "class PreprocessPipeline(IPreprocessor):\n",
        "    def __init__(self):\n",
        "        super(PreprocessPipeline, self).__init__()\n",
        "        self.transforms = nn.Sequential(\n",
        "            # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "            K.enhance.Normalize(0.0, 255.0),\n",
        "            # Redimensiona a imagem para o tamanho desejado\n",
        "            K.geometry.Resize((200, 200)),\n",
        "            # Converte a imagem RGB para escala de cinza\n",
        "            K.color.RgbToGrayscale(),\n",
        "        )\n",
        "\n",
        "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        # Normaliza a imagem para ter valores no intervalo [0, 1]\n",
        "        x = image.float() / 255.0  # Garante que a operação de divisão é realizada em float\n",
        "        # Aplica as transformações definidas em __init__\n",
        "        x = self.transforms(x)\n",
        "        # Adiciona uma dimensão de batch no início se ainda não houver\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Delaunay\n",
        "class DelaunayGraph(IGlobalFeatureStructurer):\n",
        "    @staticmethod\n",
        "    def distancePoint(p1, p2):\n",
        "        return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
        "\n",
        "    def buildConnection(self,tri):\n",
        "        mapConnection = []\n",
        "        for simplice in tri.simplices:\n",
        "            mapConnection.extend([\n",
        "                [simplice[0], simplice[1]],\n",
        "                [simplice[1], simplice[2]],\n",
        "                [simplice[2], simplice[0]]\n",
        "            ])\n",
        "        return mapConnection\n",
        "\n",
        "    def buildMapGraph(self, mapConex,featuresByPoints):\n",
        "        size = len(featuresByPoints)\n",
        "        sample = np.matrix(np.ones((size, size)) * np.inf)\n",
        "        for i in range(size):\n",
        "            sample[i, i] = 0\n",
        "        for conexao in mapConex:\n",
        "            distance = self.distancePoint(featuresByPoints[conexao[0]], featuresByPoints[conexao[1]])\n",
        "            sample[conexao[0], conexao[1]] = distance\n",
        "        return sample\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_delaunay(points, tri):\n",
        "        plt.triplot(points[:, 0], points[:, 1], tri.simplices.copy(), color='orange')\n",
        "        plt.plot(points[:, 0], points[:, 1], 'o')\n",
        "        dist = 3\n",
        "        for i, point in enumerate(points):\n",
        "            plt.text(point[0] + dist, point[1] + dist, f' {i}', color='red', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, points, featuresByPoints):\n",
        "        # Assume-se que os pontos são extraídos de uma imagem e já processados\n",
        "        tri = Delaunay(points)\n",
        "        mapConnection = self.buildConnection(tri)\n",
        "        graph = self.buildMapGraph(mapConnection,featuresByPoints)\n",
        "        return graph,tri\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "class FloydWarshall(IGlobalMatcher):\n",
        "    @staticmethod\n",
        "    def floydWarshall(graph):\n",
        "        graph = csr_matrix(graph)\n",
        "        dist_matrix, _ = floyd_warshall(csgraph=graph, directed=False, return_predecessors=True)\n",
        "        return dist_matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def match_matrix(mat_a, mat_b, threshold):\n",
        "        mat_dist = mat_b - mat_a\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            mat_dist[i, :i] = 0  # Zera a metade inferior da matriz para evitar duplicação\n",
        "        mat_dist = mat_dist * mat_dist  # Eleva as diferenças ao quadrado\n",
        "        points = 0\n",
        "        for i in range(mat_dist.shape[0]):\n",
        "            for j in range(i + 1, mat_dist.shape[0]):\n",
        "                if mat_dist[i, j] < threshold:\n",
        "                    points += 1\n",
        "        return points\n",
        "\n",
        "    def __call__(self,matrixAdj0,matrixAdj1, threshold=0.2):\n",
        "        matAdjFull0 = self.floydWarshall(matrixAdj0)\n",
        "        matAdjFull1 = self.floydWarshall(matrixAdj1)\n",
        "        simGraph = self.match_matrix(matAdjFull0,matAdjFull1,threshold)\n",
        "        return simGraph\n"
      ],
      "metadata": {
        "id": "uC7s1MDxpcXs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageComparisonPipeline:\n",
        "    __slots__ = ['preprocessor', 'local_feature_matcher', 'global_structurer', 'global_matcher']\n",
        "\n",
        "    def __init__(self, preprocessor: IPreprocessor = None, local_feature_matcher: MyLocalFeatureMatcher = None, global_structurer: IGlobalFeatureStructurer = None, global_matcher: IGlobalMatcher = None):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.local_feature_matcher = local_feature_matcher\n",
        "        self.global_structurer = global_structurer\n",
        "        self.global_matcher = global_matcher\n",
        "\n",
        "    def process_global(self,out,index):\n",
        "        pts = out[f'keypoints{index}'][out['matches'][:, index]].detach().numpy()  # Pontos de keypoints correspondentes\n",
        "        desc = out[f'descriptors{index}'][out['matches'][:, index]].detach().numpy()  # Descritores dos keypoints correspondentes\n",
        "        matrixAdj, tri = self.global_structurer(pts, desc)\n",
        "        return matrixAdj\n",
        "\n",
        "    def run(self, image1: Any, image2: Any,log=None) -> float:\n",
        "        if not all([self.preprocessor, self.local_feature_matcher, self.global_structurer, self.global_matcher]):\n",
        "            raise ValueError(\"Pipeline components are not fully set.\")\n",
        "        image1_processed, image2_processed = map(self.preprocessor, (image1, image2))\n",
        "\n",
        "        # Realiza o matching de características entre as duas imagens processadas\n",
        "        out = self.local_feature_matcher(image1_processed,image2_processed)\n",
        "\n",
        "        if log is not None and log in ('DEBUG','INFO'):\n",
        "          print(out['keypoints1'].shape,type(out['keypoints1']),'\\n',\n",
        "                out['lafs1'].shape,type(out['lafs1']),'\\n',\n",
        "                out['descriptors1'].shape,type(out['descriptors1']),'\\n',\n",
        "                out['matches'].shape,)\n",
        "\n",
        "        if log is not None and log in ('DEBUG'):\n",
        "          draw = MyDrawMatcher()\n",
        "          draw(image1_processed,image2_processed,out)\n",
        "        try:\n",
        "          matrixAdj0 = self.process_global(out,0)\n",
        "          matrixAdj1 = self.process_global(out,1)\n",
        "          score = self.global_matcher(matrixAdj0, matrixAdj1, threshold=0.005)\n",
        "        except:\n",
        "          score = 0\n",
        "        return score\n",
        "\n",
        "\n",
        "class ImageSearchEvaluator:\n",
        "    def __init__(self, pipeline: ImageComparisonPipeline):\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def data_augmentation(self, dataset: List[str]) -> List[str]:\n",
        "        # Implemente a operação de data augmentation para gerar o dataset de inspeção\n",
        "        dataset_de_inspecao = dataset  # Aqui você deve implementar a lógica de data augmentation\n",
        "        return dataset_de_inspecao\n",
        "\n",
        "    def evaluate_accuracy(self, dataset_de_referencia: List[str], dataset_de_inspecao: List[str]) -> float:\n",
        "        total_correspondencias_corretas = 0\n",
        "        total_correspondencias = 0\n",
        "\n",
        "        for indice_imagem_inspecao, imagem_de_inspecao in enumerate(dataset_de_inspecao):\n",
        "            # Realize uma busca sobre o dataset de referência para encontrar a correspondência\n",
        "            melhor_correspondencia = None\n",
        "            melhor_score = float('-inf')\n",
        "            print(\"fase {}\".format(indice_imagem_inspecao))\n",
        "            for indice_imagem_referencia, imagem_de_referencia in enumerate(dataset_de_referencia):\n",
        "                try:\n",
        "                    similarity_score = self.pipeline.run(imagem_de_referencia, imagem_de_inspecao)\n",
        "                    if similarity_score > melhor_score:\n",
        "                        melhor_score = similarity_score\n",
        "                        melhor_correspondencia = indice_imagem_referencia\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "                print(indice_imagem_inspecao,melhor_correspondencia)\n",
        "\n",
        "            # Verifique se a correspondência encontrada está correta\n",
        "            if melhor_correspondencia == indice_imagem_inspecao:\n",
        "                total_correspondencias_corretas += 1\n",
        "            total_correspondencias += 1\n",
        "\n",
        "\n",
        "        # Compute a acurácia do pipeline na busca das correspondências corretas\n",
        "        accuracy = total_correspondencias_corretas / total_correspondencias if total_correspondencias > 0 else 0\n",
        "        return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "vTPZO0Mwuucx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kornia.feature import LocalFeatureMatcher,GFTTAffNetHardNet\n",
        "from kornia import tensor_to_image  # Importa função para converter tensor PyTorch em imagem numpy\n",
        "\n",
        "# Carregar as duas imagens utilizando torchvision\n",
        "# image1 = torchvision.io.read_image(\"image1.jpg\").float()\n",
        "# image2 = torchvision.io.read_image(\"image2.jpg\").float()\n",
        "\n",
        "preprocess = PreprocessPipeline()\n",
        "drawMatch = MyDrawMatcher()\n",
        "# Configura o extrator de características locais e o matcher de descritores\n",
        "customFeatureMatcher = MyLocalFeatureMatcher(\n",
        "    GFTTAffNetHardNet(25), K.feature.DescriptorMatcher('smnn', 0.95)\n",
        ")\n",
        "\n",
        "delaunayG = DelaunayGraph()  # Inicializa o construtor de grafo de Delaunay\n",
        "# Compara as matrizes de distância dos dois grafos\n",
        "floyd = FloydWarshall()"
      ],
      "metadata": {
        "id": "lpNTUTLjHG5J"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exemplo de uso:\n",
        "\n",
        "pipeline = ImageComparisonPipeline(preprocess,customFeatureMatcher,delaunayG,floyd)\n",
        "\n",
        "# try:\n",
        "#     similarity_score = pipeline.run(image1, image2, log='DEBUG')\n",
        "#     print(f\"Similarity Score: {similarity_score}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "evaluator = ImageSearchEvaluator(pipeline)\n",
        "# Suponha que você tenha um dataset de referência e queira usar o mesmo dataset para gerar o dataset de inspeção\n",
        "# dataset_de_referencia = [image1,image2]  # Lista de tensores de imagem representando o dataset de referência\n",
        "# dataset_de_inspecao = evaluator.data_augmentation(dataset_de_referencia)\n",
        "\n",
        "# Calcule a acurácia do pipeline na busca das correspondências corretas\n",
        "# accuracy = evaluator.evaluate_accuracy(dataset_de_referencia, dataset_de_inspecao)\n",
        "# print(f\"Acurácia do pipeline na busca das correspondências corretas: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "M6vqE1L8s_fN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import kornia.augmentation as KA\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Flowers102Dataset(Dataset):\n",
        "    def __init__(self, root='./data', split='train', download=True):\n",
        "        self.dataset = datasets.Flowers102(root=root, split=split, download=download, transform=transforms.ToTensor())\n",
        "        self.pre_transforms = transforms.Compose([\n",
        "            transforms.Resize((256,256))  # Redimensionamento\n",
        "        ])\n",
        "        # Transformações que serão aplicadas para obter a versão transformada\n",
        "        self.transforms = torch.nn.Sequential(\n",
        "            KA.RandomHorizontalFlip(p=0.5),\n",
        "            KA.RandomVerticalFlip(p=0.5),\n",
        "            # KA.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            KA.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.95, 1.05), p=0.5)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.dataset[index]\n",
        "\n",
        "        # Armazenando a versão original (antes das transformações)\n",
        "        original_data = self.pre_transforms(data)\n",
        "\n",
        "        # Aplicando transformações para obter a versão transformada\n",
        "        transformed_data = self.transforms(original_data.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # Retornando ambas as versões da imagem junto com o target/label\n",
        "        return original_data, transformed_data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "# DataLoader\n",
        "flowers_loader = DataLoader(Flowers102Dataset(), batch_size=24, shuffle=True)"
      ],
      "metadata": {
        "id": "6pLVS531AVIC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSearchEvaluator:\n",
        "    def __init__(self, pipeline: ImageComparisonPipeline):\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def evaluate_accuracy(self, flowers_loader) -> float:\n",
        "        total_correspondencias_corretas = 0\n",
        "        total_correspondencias = 0\n",
        "        try:\n",
        "          for batch_idx, (original_data, transformed_data, target) in enumerate(flowers_loader):\n",
        "            print(\"batch_idx {}  -  {}\".format(batch_idx,original_data.shape))\n",
        "            for indice_imagem_inspecao, imagem_de_inspecao in enumerate(transformed_data):\n",
        "                # Realize uma busca sobre o dataset de referência para encontrar a correspondência\n",
        "                melhor_correspondencia = None\n",
        "                melhor_score = float('-inf')\n",
        "                with medir_tempo('Processamento concluído'):\n",
        "                  for indice_imagem_referencia, imagem_de_referencia in enumerate(original_data):\n",
        "                      try:\n",
        "                          similarity_score = self.pipeline.run(imagem_de_inspecao, imagem_de_referencia)\n",
        "                          print(\"teste {}/{} - {}\".format(indice_imagem_inspecao,indice_imagem_referencia,similarity_score))\n",
        "                          if similarity_score > melhor_score:\n",
        "                              melhor_score = similarity_score\n",
        "                              melhor_correspondencia = indice_imagem_referencia\n",
        "                      except Exception as e:\n",
        "                          print(f\"Erro durante a execução do pipeline: {e}\")\n",
        "\n",
        "                # Verifique se a correspondência encontrada está correta\n",
        "                if melhor_correspondencia == indice_imagem_inspecao:\n",
        "                    total_correspondencias_corretas += 1\n",
        "                total_correspondencias += 1\n",
        "                print(\"Match {} -> {}\".format(indice_imagem_inspecao,melhor_correspondencia))\n",
        "        finally:\n",
        "          # Compute a acurácia do pipeline na busca das correspondências corretas\n",
        "          accuracy = total_correspondencias_corretas / total_correspondencias if total_correspondencias > 0 else 0\n",
        "          print(\"accuracy -> \",accuracy)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "evaluator = ImageSearchEvaluator(pipeline)\n",
        "evaluator.evaluate_accuracy(flowers_loader)"
      ],
      "metadata": {
        "id": "YtyMXLAzAaIn",
        "outputId": "897cfcd4-b979-440c-e2fb-73c9d402559b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_idx 0  -  torch.Size([24, 3, 256, 256])\n",
            "teste 0/0 - 73\n",
            "teste 0/1 - 0\n",
            "teste 0/2 - 0\n",
            "teste 0/3 - 0\n",
            "teste 0/4 - 0\n",
            "teste 0/5 - 5\n",
            "teste 0/6 - 0\n",
            "teste 0/7 - 0\n",
            "teste 0/8 - 0\n",
            "teste 0/9 - 0\n",
            "teste 0/10 - 6\n",
            "teste 0/11 - 1\n",
            "teste 0/12 - 0\n",
            "teste 0/13 - 0\n",
            "teste 0/14 - 0\n",
            "teste 0/15 - 2\n",
            "teste 0/16 - 0\n",
            "teste 0/17 - 0\n",
            "teste 0/18 - 0\n",
            "teste 0/19 - 0\n",
            "teste 0/20 - 0\n",
            "teste 0/21 - 0\n",
            "teste 0/22 - 0\n",
            "teste 0/23 - 0\n",
            "Processamento concluído: 70.952663127 segundos\n",
            "Match 0 -> 0\n",
            "teste 1/0 - 0\n",
            "teste 1/1 - 3\n",
            "teste 1/2 - 0\n",
            "teste 1/3 - 0\n",
            "teste 1/4 - 0\n",
            "teste 1/5 - 0\n",
            "teste 1/6 - 0\n",
            "teste 1/7 - 0\n",
            "teste 1/8 - 2\n",
            "teste 1/9 - 0\n"
          ]
        }
      ]
    }
  ]
}